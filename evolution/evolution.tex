\chapter{Software Evolution in Real-world}
\label{chap:evolution}

The multi-version execution approach is based on two key assumptions.  First,
that new bugs are being introduced during the software evolution and
maintenance process even in well-tested code. Second, that during software
evolution, the externally observable behaviour of the applications remains
relatively stable, especially between minor revisions (\ie security and bug
fixes).  While there is a lot of first hand and anecdotal evidence in support
of these assumptions, despite the key role that software evolution plays in the
application life cycle, it is surprising how few empirical studies one can find
in the research literature regarding the evolution of the \emph{execution} of
real systems.

Software repositories provide rich information about the construction and
evolution of software systems. While static data extracted from software
repositories have been extensively studied, dynamic metrics concerning the
execution of the software have received much less attention, due to the
inherent difficulty of running and monitoring a large number of software
versions.

In this chapter, we present an empirical study concerning both static and
dynamic metrics which aims to answer some of the questions related to software
evolution. To perform this study, we have built a flexible infrastructure that
can be used to run each version of a system in isolation and collect static and
dynamic software metrics from the test suite execution. We consider the tests
to be an (imperfect) proxy for a real-world execution.

We have used this infrastructure to examine how code and tests co-evolve in
\numSystems popular open-source systems. We report the main characteristics of
software patches, and analyse the program evolution from both the source code
and the external behaviour perspective. We assess the impact of non-determinism
on the execution of test suites. We also analyse the code and patch coverage,
and investigate whether the coverage of code containing bugs and bug fixes is
lower or higher than average; the latter of the two would provide evidence that
even well tested code still has bugs, in support of our technique.

\input{evolution/design}
\input{evolution/study}
\input{evolution/coverage}
\input{evolution/external}
\input{evolution/threaths}
\input{evolution/discussion}
\input{evolution/summary}
