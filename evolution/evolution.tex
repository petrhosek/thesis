\chapter{Software Evolution in Real-world}
\label{chap:evolution}

Software repositories provide rich information about the
construction and evolution of software systems.  While static data
%extracted from  software repositories 
%such as those related to API usage, clone detection, or bug reporting 
that can be mined directly from version control systems
has been extensively studied, dynamic metrics
concerning the execution of the software have received much less
attention, due to the inherent difficulty of running and monitoring
a large number of software versions.

While static metrics can provide useful insights into the construction
and evolution of software, there are many software engineering aspects
which require information about software executions.  For example, the
research community has invested a lot of effort in designing
techniques for improving the testing of software patches, ranging from
test suite prioritisation and selection
algorithms~\cite{harrold:test-redundancy,test-pri,Rothermel96analyzingregression}
to program analysis techniques for test case generation and bug
finding~\cite{diff-symex,directed-test-augmen:09,express,directed-symex11,babic11,directed-incr-symex11,patch:spin12,interaction-changes13}
to methods for surviving errors introduced by patches at
runtime~\cite{mx}.

Many of these techniques depend on the existence of a manual test
suite, sometimes requiring the availability of a test exercising the
patch~\cite{onlinevalidation,tachyon12}, sometimes making assumptions
about the stability of program coverage or external behaviour over
time~\cite{cov_regr97,mx}, other times using it as a starting point
for
exploration~\cite{zesti,pretex,sage,test-augmentation:genetic-vs-concolic},
and often times employing it as a baseline for
comparison~\cite{klee,dotnet-random-test08,semantic-fp-testing12,mutation-tests-oracle12}.
However, despite the key role that test suites play in software
testing, it is surprising how few empirical studies one can find in
the research literature regarding the co-evolution of test suites and
code and their impact on the \textit{execution} of real systems.

In this chapter, we present \covrig, a flexible infrastructure that can
be used to run each version of a system in isolation and collect
static and dynamic software metrics, using a lightweight virtual
machine environment that can be deployed on a cluster of local or
cloud machines.

We use \covrig to conduct an empirical study examining how code and
tests co-evolve in \numSystems popular open-source systems.  We
report the main characteristics of software patches, analyse the
evolution of program and patch coverage, assess the impact of
nondeterminism on the execution of test suites, and investigate
whether the coverage of code containing bugs and bug fixes is higher
than average.

We use \covrig to conduct an empirical study examining how programs
evolve in terms of code, tests and coverage.  More precisely, we have
analysed the evolution of \numSystems popular software systems with a
rich development history over a combined period of \numYears years,
with the goal of answering the following list of research questions
(RQs):

\begin{itemize}
\item[\textssc{RQ1}] \textit{\rqone}
            Are coding and testing continuous, closely linked
            activities?  Or do periods of intense development
            alternate with periods of testing?

\item[\textssc{RQ2}] \textit{\rqtwo}
            %% What is the distribution of patches that only modify
            %% executable code, only tests or both?  
            Are most code
            patches accompanied by a new or modified test case?  How
            many patches modify neither executable code nor tests?
           
\item[\textssc{RQ3}] \textit{\rqthree}
            Are most patches small?  
            How many different parts of the code does a patch touch?
            What is the median number of lines, hunks and
            files affected by a patch?

\item[\textssc{RQ4}] \textit{\rqfour}  Do tests fail non-deterministically?
            Does running the test suite multiple times cover different
            lines of code?
%If not, how many lines are nondeterministically covered on average?

\item[\textssc{RQ5}] \textit{\rqfive}
            Does the overall coverage increase steadily over time, or
            does it remain constant?  Are there revisions that
            significantly increase or decrease coverage?

\item[\textssc{RQ6}] \textit{\rqsix}
            What fraction of a patch is covered by the regression test
            suite?  Does patch coverage depend on the size of the
            patch?

\item[\textssc{RQ7}] \textit{\rqseven}  Are
            tests exercising recent patches added shortly after the
            patch was submitted?  If so, how significant is this
            latent patch coverage?

\item[\textssc{RQ8}] \textit{\rqeight}
            Are most fixes thoroughly exercised by the regression
            suite?  How many fixes are entirely executed?
            %% Are most bug fixes accompanied by a test case?  
            %% Do developers invest more effort into testing fixes than
            %% other types of patches?

\item[\textssc{RQ9}] \textit{\rqnine}
            Is code that contains bugs exercised less than other changes?
            Is coverage a reasonable indicator of code quality? 

%\item[\bf RQ10:] \textbf{Does buggy code have lower than average coverage?}
\end{itemize}

\section{\covrig Infrastructure}
\label{sec:design}
\input{evolution/design}

\section{Test and Coverage Evolution}
\label{sec:coverage-evolution}
\input{evolution/coverage}

\section{External Behavior Evolution}
\label{sec:behavior-evolution}
\input{evolution/external}

\section{Threats to Validity}
\label{evolution:threats}

The main threat to validity in our study regards the generalisation of
our results.  The patterns we have observed in our data may not
generalise to other systems, or even to other development periods
for the same systems.  However, we regard the selected systems to be
representative for open-source C/C++ code, and the analysis period was
chosen in an unbiased way, starting with the current version at the
time of our experiments.

Errors in the software underlying our framework could have interfered with
our experiments. Both Docker and Linux Containers were under development
and not recommended for use in production systems at the time of our study.
Furthermore, in case of some applications, we have observed test failures
caused by the AUFS filesystem used by Docker. However, we have thoroughly
investigated these failures and we believe they did not affect the results
presented in our study.

Given the large quantity of data that we collected from a large number
of software revisions, errors in our scripts cannot be excluded.
%However, we and the ISSTA artifact evaluation committee have
%thouroughly checked our results and scripts, and we are making our
%framework and data available for further validation.
% In addition, our artifact submission was
% found to exceed expectations by the ISSTA artifact evaluation
% committee.

\section{Discussion}
\label{evolution:discussion}

We believe this study provides evidence in support of the assumption that
changes to the external behaviour are relatively small, which is encouraging
for the viability and applicability of the multi-version execution approach.
