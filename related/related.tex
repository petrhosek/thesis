\chapter{Related Work}
\label{chap:related}

This chapter provides an overview of previous work in the area.  We first
discuss other scenarios which involve the execution of multiple program
versions, and then discuss related work in the virtualization space.

\section{N-version programming}

The original idea of concurrently running multiple versions of the same
application was first explored in the context of $N$-version programming, a
software development approach introduced in the 1970's in which multiple teams
of programmers independently develop functionally equivalent versions of the
same program in order to minimise the risk of having the same bugs in all
versions~\cite{chen1995}. During runtime, these versions are executed in
parallel atop $N$-version execution environment (NVX)  in order to improve the
fault tolerance of the overall. Both version generation and the synchronization
mechanism required manual effort.

%system and majority voting is used to continue in the best possible way when a
%divergence occurs.

%N-version programming was introduced in the seventies by Chen and
%Avizienis~\cite{chen1995}.  The core idea was to have multiple teams
%of programmers develop the same software independently and then run
%the produced implementations in parallel in order to improve the fault
%tolerance of the overall system.  Both version generation and the
%synchronization mechanism required manual effort.

\subsection{Multi-variant execution}

Cox \etal~\cite{cox2006} propose a general framework for increasing application
security by running in parallel several automatically-generated diversified
variants of the same program.  The technique was implemented in two prototypes,
one in which variants are run on different machines, and one in which they are
run on the same machine and synchronised at the system call level, using a
modified Linux kernel.  By using multiple variants with potentially disjoint
exploit sets, the proposed approach helps applications survive certain classes
of security vulnerabilities such as buffer overflows, as attackers would need
to simultaneously compromise all variants.

Berger \etal~\cite{diehard06} described a different approach which uses heap
over-provisioning and full randomisation of object placement and memory reuse
to run multiple replicas and reduce the likelihood that a memory error will
have any effect
%uses address space layout randomization to generate multiple replicas that are
%executed concurrently. This approach is combined with randomization and
%replication to provide memory error tolerance.
The goal of their work is to increase reliability by tolerating memory errors
in exchange for space costs and execution time.

%A different solution described by Shye \etal in~\cite{shye2009} runs multiple
%instances of the same version of a single application aiming to overcome
%transient failures by monitoring and comparing their execution on the level of
%kernel system calls.  However, this solution does not aim to overcome such
%failures.

Within this paradigm, the Orchestra framework~\cite{orchestra09} uses a
modified compiler to produce two versions of the same application with stacks
growing in opposite directions, runs them in parallel on top of an unprivileged
user-space monitor, and raises an alarm if any divergence is detected to
protect against stack-based buffer overflow attacks.  This work has been
further improved in~\cite{orchestra11} with new method for signal delivery
accompanied with detailed analysis of the benchmark characteristics. However,
their approach is still limited to two versions running in lockstep with only
difference being the opposite growing stack.

Trachsel \etal~\cite{trachsel10} use the multi-variant execution approach to
increase performance, where program variants are obtained by using different
(compiler) optimisations and algorithms.  The goal is to increase the overall
system performance by always selecting the variant which finishes its execution
first. Thereby, no synchronisation across variants is needed.

More recently, researchers have proposed additional multi-variant techniques
that fit within the $N$-version programming paradigm, \eg by employing
complementary thread schedules to survive concurrency
errors~\cite{compl-schedules11}, or using genetic programming to automatically
generate a large number of application variants that can be combined to reduce
the probability of failure or improve various non-functional
requirements~\cite{gismoe}. Yumerefendi \etal~\cite{tightlip} run multiple
sandboxed copies of the same process to detect potential privacy leaks and to
enforce access control and privacy management; Shye \etal~\cite{shye2009} use
multiple instances of the same application in order to overcome transient
hardware failures. Cadar \etal have also argued that automatically generated
software variants are a good way for exploiting the highly parallel nature of
modern hardware platforms~\cite{multiplicity}.

%Also ManySAT~\cite{manysat} %% \todo{\ldots missing rest of the sentence}

There are two key differences between our approach and the work discussed
hereinbefore.  First, we do not rely on automatically-generated variants, but
instead run in parallel existing software versions, which raises a number of
different technical challenges.  This means that in our case, the variants are
not semantically equivalent, this eliminates the challenge of generating
diversified variants and creates opportunities in terms of recovery from
failures, but also introduces additional challenges in terms of synchronising
the execution of the different versions.  Second, this body of work has mostly
focused on detecting divergences, while our main concern is to \textit{survive}
them (keeping all versions alive), in order to increase both the security and
availability of the overall application.

%Recent work on NVX systems has moved in the direction of
%opportunistically using existing versions---\eg different browser
%implementations~\cite{cocktail} or different software
%revisions~\cite{mx}---or automatically synthesizing them---\eg by
%varying the memory layout~\cite{cox2006,diehard06}, or the direction
%of stack growth~\cite{orchestra09}.  Significant effort has also been
%expended on running multiple versions in parallel in the context of
%online and offline
%testing~\cite{back-to-back90,onlinevalidation,bandaid-patch07,tachyon12}.
%\nx targets NVX systems that use
%system-call level synchronization and is oblivious to the way in which
%the versions are generated.

Recent NVX systems that synchronize versions at the level of system
calls use either the \ptrace interface~\cite{orchestra09,tachyon12,mx}
or kernel modifications~\cite{cox2006} to implement monitors.  As
discussed, \stt{ptrace}-based systems incur an unacceptable overhead
on I/O-bound applications---for instance, \tachyon~\cite{tachyon12}
reports an overhead of \tachyonLighttpd on \lighttpd and \mx~\cite{mx}
achieved an overhead of up to \mxRedis in one of the Redis
experiments.  By contrast, kernel-based systems~\cite{cox2006} achieve
overheads competitive to \varan~\footnote{although we were not able to compare
directly with \cite{cox2006}, as they use a different benchmark and a
single-core machine}---but the major downside of kernel-level approaches are
the increase in the size of the trusted computing base (TCB), the additional
privileges required for deployment, and the difficulty of maintaining the
kernel patches.  Finally, all existing NVX systems operating at the level of
system calls, both user- and kernel-level, require lockstep execution, which
introduces significant limitations both in terms of performance and
flexibility, as discussed in detail in Sections~\ref{sec:coordination} and
\ref{sec:patternmatching}.

\subsection{Multi-version Execution}

Cook and Dage~\cite{cook:icse99} proposed a multi-version framework for
upgrading components.  Users formally specify the specific input subdomain that
each component version should handle, after which versions are run in parallel
and the output of the version whose domain includes the current input is
selected as the overall output of the computation.  The system was implemented
at the level of leaf procedures in the Tcl language.  The key difference with
\mx is that this framework requires a formal description of what input domain
should be handled by each version; in comparison, \mx targets crash bugs and is
fully automatic.  Moreover, \mx's goal is to have all versions alive at all
times, so crash recovery plays a key role.  Finally, \mx has to carefully
synchronise access to shared state, which is not an issue at the level of Tcl
leaf procedures.

%A possible realization of the original N-version programming paradigm is
An approach closer to the original N-version programming paradigm is
Cocktail~\cite{cocktail}, which proposes the idea of running different web
browsers in parallel under the assumption that any two of them are unlikely to
be vulnerable to the same attacks.  Compared to \mx and other techniques
inspired by the N-version programming paradigm, Cocktail's task is simplified
by exclusively targeting web browsers, which implement common web standards.

%\subsection{Software testing}

The multi-version execution approach has been in the past successfully used for
testing. Back-to-back testing~\cite{back-to-back90}, where the same input is
sent to different variants or versions of an application and their outputs
compared for equivalence, has been used since the 1970s.  Band-aid
patching~\cite{bandaid-patch07} proposed an online patch testing system that
splits execution before a patch, and then retroactively selects one code
version based on certain criteria.  More recently, delta
execution~\cite{onlinevalidation} proposed to run two different versions of a
single application, splitting the execution at points where the two versions
differ, and comparing their behaviour to test the patch for errors and validate
its functionality.  Similarly, Tachyon~\cite{tachyon12} is an online patch
testing system
%developed in recent independent work 
in which the old and the new version of an application are run concurrently;
when a divergence is detected, the options are to either halt the program, or
to create a manual rewrite rule specifying how to handle the divergence.

The idea of running multiple executions concurrently has also been used in an
offline testing context.  For instance, d'Amorin \etal~\cite{delta-exec-oop}
optimise the state-space exploration of object oriented code by running the
same program on multiple inputs simultaneously, while Kim
\etal~\cite{shared-exec12} improve the testing of software product lines by
sharing executions across a program family.

By comparison with this body of work, our focus is on managing
divergences across software versions at runtime in order to keep the
application running, and therefore runtime deployment and automatic
crash recovery play a central role in both \mx and \nx.

\section{Software updates}

Closely related to the execution of multiple versions is the management of
multiple versions, environment and software updates, such as deciding when to
upgrade, applying updates, \etc Previous work on improving the software update
process has looked at different aspects related to managing and deploying new
software versions. 

\subsection{Dynamic software updates}

Dynamic software updating (DSU) systems are concerned with the problem of
updating programs while they are running. These systems have been implemented
both in kernel-space~\cite{k42,dynamos,ksplice,proteos} and in
user-space~\cite{opus,ginseng,polus,upstare,ekiden,kitsune}.

The dynamic update process typically consists of two stages---code update and
state transfer---with different approaches available.

Ginseng~\cite{ginseng} and K42~\cite{k42} employ indirection to enable the code
update. This simplifies the code update mechanism, but the indirection
introduces performance overhead during normal execution. Ginseng uses a
specialized compiler which creates a ``dynamically updateable program'' where
all function symbols are replaced with function pointers and all direct
function calls are replaced with indirect call through this pointer. During
update, these pointers are updated to point to the new version of the code,
which is loaded using \lstinline`dlopen`. K42 operating system has a
per-address-space object translation table used for all object invocations.
When the new kernel module is loaded, the translation table entries are updated
to point to the new version.

OPUS~\cite{opus}, DynAMOS~\cite{dynamos}, POLUS~\cite{polus} and
Ksplice~\cite{ksplice} use binary rewriting instead of indirection to replace
the entry point of the functions being updated with a jump to a trampoline.
Compared to indirection, this approach does not require the use of specialized
compiler, but it is highly platform-dependent. We use the same approach in \nx
to handle virtual system calls (\S\ref{sec:vsyscall}).

The aforementioned systems treat individual functions or objects as the unit of
code for updates. Such systems are incapable of handling functions that rarely
end (\eg \lstinline`main` or functions that contain event-handling loops). This
was addressed by systems such as UpStare~\cite{upstare}, Ekiden~\cite{ekiden}
and Kitsune~\cite{kitsune}, which update the code by loading an entirely new
program instead of replacing individual functions. After the new version has
been loaded, either through \lstinline`fork-exec` or through
\lstinline`dlopen`, the execution needs to be restarted from the previous
point. UpStare uses stack reconstruction to achieve this by replacing the stack
frames for old functions with their new versions. Ekiden and Kitsune on the
other hand rely on manual approach requiring the programmer to direct the
execution into the equivalent update point in the new version.

%\textsc{PROTEOS}~\cite{proteos}.

Dynamic software updating (DSU) systems such as Ginseng~\cite{ginseng},
UpStare~\cite{upstare} or Kitsune~\cite{kitsune} are concerned with the problem
of updating programs while they are running.

Opposed to \mx, in DSU systems the two versions co-exist only for the duration
of the software update, but DSU and the \rem component of \mx face similar
challenges when switching execution from one version to another.  We hope that
some of the technique developed in DSU research will also benefit the recovery
mechanism of \mx and vice versa.

%Dynamic software updating (DSU) systems such as Ginseng~\cite{ginseng},
%UpStare~\cite{upstare} or Kitsune~\cite{kitsune} are concerned with the problem
%of updating programs while they are running.  As opposed to \mx, the two
%versions co-exist only for the duration of the software update, but DSU and the
%\rem component of \mx face similar challenges when switching execution from one
%version to another.  We hope that some of the technique developed in DSU
%research will also benefit the recovery mechanism of \mx and vice versa.

\subsection{Update management and distribution}

Prior work on improving software updating has looked at different aspects
related to managing and deploying new software versions.

Beattie~\etal~\cite{beattie2002} have considered the issue of timing the application
of security updates---patching too early could result in breaking the system by
applying broken patch, patching too late could on the other hand lead to risk
of penetration by an attacker exploiting a well known security issue. Using the
cost functions of corruption and penetration, based upon real world empirical
data, they have shown that 10 and 30 days after the patch's release date are
the optimal times to apply the patch to minimize the risk of defective patch.
Such delay still opens a lot of opportunities for potential attackers.

Crameri \etal~\cite{crameri:updates} proposed a framework for staged
deployment. This framework integrates upgrade deployment, user-machine testing
and problem reporting into the overall upgrade process. The framework itself
clusters user machines according to their environment and software updates are
tested across clusters using several different strategies allowing for the
deployment of complex upgrades.

Solution inspired by $N$-version programming been proposed by Michael
Franz~\cite{unibus:nspw10}---instead of executing multiple versions in parallel,
the author proposed to distribute unique version of every program to every
user. Such versions should be created automatically by a ``multicompiler'',
and distributed to users through ``App Store''. This would increase security as
it would be much more difficult to generate attack vectors by reverse
engineering of security patches for these diversified versions.

%To make such a solution work in practice, the way to manage large number of
%different versions and the overall update process is needed.
%Crameri~\etal~\cite{crameri:updates} proposed a framework for staged deployment
%which integrates upgrade deployment, user-machine testing and problem reporting
%into the overall upgrade process. The framework itself clusters users'
%machines according to their environment, tests the upgrades using cluster
%representatives and allows deployment of complex upgrades.

%Beattie~\etal~\cite{beattie2002} considered the issue of timing the application
%of security updates---patching too early could result in breaking the system by
%applying broken patch, patching too late could on the other hand lead to risk
%of penetration by an attacker exploiting a well known security issue. Using the
%cost functions of corruption and penetration, based upon real world empirical
%data, they have shown that 10 and 30 days after the patch's release date are
%the optimal times to apply the patch to minimize the risk of defective patch.
%Such delay still opens a lot of opportunities for potential attackers.

In relation to this work, \mx tries to ease the decision of applying a software
update, because incorporating a new version should only increase the security
and reliability of the overall multi-version application.  However, in practice
the number of versions that can be run in parallel is dictated by the number of
available resources (\eg the number of available CPU cores), so effective
update strategies are still needed in this context, and work in this area could
provide helpful solutions to this problem.

%In relation to this work, \mx tries to encourage users to always apply
%a software update, but it would still benefit from effective update
%strategies to decide what versions to keep when resources are
%limited.

Many large-scale services, such as Facebook and Flickr use a {\it continuous
deployment} approach, where new versions are continuously released to
users~\cite{johnson2009,flickr}, but each version is often made accessible only
to a fraction of users to prevent complete outage in case of newly introduced
errors.  While this approach helps minimize the number of users affected by new
bugs, certain bugs may manifest themselves only following prolonged operation,
after the release has been deployed to the entire user base.  We believe our
proposed approach is complementary to continuous deployment, and could be
effectively combined with it.

\input{related/sandboxing}
\input{related/record}
\input{related/evolution}

%\subsection{Execution of Multiple Versions}

%The idea of concurrently running multiple versions (or a \emph{multi-version
%execution}) of the same application was first explored in the context of
%$N$-version programming, a software development methodology introduced in the
%seventies in which multiple teams of programmers develop functionally
%equivalent versions of the same program in order to minimize the risk of having
%the same bugs in all versions~\cite{chen1995}. Furthermore, the use of an
%execution environment responsible for running the $N$-version programs and
%choosing one of the outputs was proposed.
  
%This idea has been followed up in~\cite{cox2006}, which proposes the use of
%automatically generated {\it diversified variants} of the same program to
%increase application security. Through the use of multiple variants with
%potentially disjoint exploit sets, the proposed approach makes it difficult to
%exploit a large class of security vulnerabilities such as buffer overflow, as
%attackers would need to simultaneously compromise all variants.

%The idea has been implemented in \cite{orchestra09}, which uses a modified
%compiler to produce two versions of the same application with stacks growing in
%opposite directions, together with an execution environment that monitors both
%variants at runtime, checking for any discrepancies in system calls made by the
%variants that would indicate a successful security attack on one of the
%replicas. This work has been further improved in \cite{orchestra11} with new
%method for signal delivery accompanied with detailed analysis of the benchmark
%characteristics. However, their approach is still limited to two versions
%running in lockstep with only difference being the opposite growing stack.
%Most importantly, whenever divergence between the two versions is detected,
%they are immediately stopped.

%The multi-version execution idea has been also used for different purposes ---
%Berger et al. describe a similar approach that uses address space layout
%randomization to generate multiple replicas that are executed
%concurrently~\cite{diehard06}. This approach is combined with randomization
%and replication to provide memory error tolerance. The goal of their work is to
%increase reliability by tolerating memory errors in exchange for space costs
%and execution time.

%Solution described in~\cite{shye2009} runs multiple instances of the same
%version of a single application aiming to overcome transient failures by
%monitoring and comparing their execution on the level of kernel system calls.
%However, this solution does not aim to overcome such failures.

%Running different versions of an application in parallel has also been used to
%test and validate software patches.  In~\cite{onlinevalidation}, two different
%versions of a single application are run in parallel, splitting the execution
%at points where the two versions differ, and comparing their results to test
%the patch for errors and validate its functionality. Whenever one of the two
%versions crashes, a bug is reported. This approach is limited only to a
%specific categories of patches such as refactoring or changes to rarely used
%paths (\eg error handlers). Moreover, this approach is only targeted towards
%on-line validation, not as a generally usable runtime environment.

%Trachsel et al.  use a similar approach to increase performance, where program
%variants are obtained by using different (compiler) optimizations and
%algorithms~\cite{trachsel10}.  The goal is to increase the overall system
%performance by always selecting the variant which finishes its execution first.
%Thereby, no synchronization across variants is needed.

%\subsection{Multi-version Environment and Update Management}

%Closely related to the execution of multiple versions is the management of
%multiple versions, environment and software updates, such as deciding when to
%upgrade, applying updates, \etc

%Solution, which in some sense resembles the multi-version execution idea, has
%been proposed by Michael Franz in~\cite{franz2010} --- instead of executing
%multiple versions in parallel, he proposes to distribute unique version of
%every program to every user. Such versions should be created automatically by
%a ``multicompiler'' and distributed to users through ``App Store''. This would
%increase security as it would be much more difficult to generate attack
%vectors by reverse engineering of security patches for these diversified
%versions.

%To make such a solution work in practice, the way to manage large number of
%different versions and the overall update process is needed. This idea has
%been explored in~\cite{crameri:updates}. This framework integrates upgrade
%deployment, user-machine testing and problem reporting into the overall
%upgrade process.  The framework itself clusters users' machines according to
%their environment, tests the upgrades using cluster representatives and allows
%deployment of complex upgrades.

%Beattie et al. showed in~\cite{beattie2002} that the timing of security patch
%applying can be of critical importance. Patching too early could result in
%breaking the system by applying broken patch, patching too late could on the
%other hand lead to risk of penetration by an attacker exploiting a well known
%security issue. Using the cost functions of corruption and penetration, based
%upon real world empirical data, they have shown that 10 and 30 days after the
%patch's release date are the optimal times to apply the patch to minimize the
%risk of defective patch. Such delay still opens a lot of opportunities for
%potential attackers.

%We believe that our approach can decrease this time virtually to zero
%eliminating the possibility of penetration while retaining the reliability of
%the original version.

%\begin{structure*}
%\item MonDe: Safe Updating through Monitored Deployment of New Component Versions
%\item Towards A Self-Managing Software Patching Process Using Balck-Box Persistent Manifests
%\item Predicting Problems Caused by Component Upgrades
%\item The Cracker Patch Choice: An Analysis of Post Hoc Security Techniques
%\item Yesterday, my program worked. Today, it does not. Why?
%\end{structure*}

%\subsection{Application Sandboxing and Software Fault Isolation}

%Multi-version execution requires some form of application sandboxing and
%software fault isolation. The goal is to isolate the running application from
%underlying system and especially, to prevent software failures from affecting
%the rest of the system including other applications.

%The use of software-based fault isolation for executing untrusted code has
%been described already in~\cite{wahbe1993} for the RISC machines with simple
%instruction set. On the other hand, the first effective implementation of
%software fault isolation for the CISC architecture has been shown much later
%in~\cite{mccamant2006}.

%% Control flow integrity?

%Douceur et. al shown in~\cite{douceur08} how to use sandboxing to enable
%leveraging of existing libraries and programs on the web. To achieve that,
%they used application-level virtualization; their implementation use system
%call mediation together with their own platform abstraction layer to run each
%library or program in so-called \emph{picoprocesses} which can be seen as
%stripped down virtual machine. The biggest downside of this approach is the
%need for code modifications which significantly reduces usability of this
%approach.

%Similar idea has been implemented by Yee et al. in~\cite{nacl} as an
%extension to Google Chrome web browser providing sandbox for untrusted native
%x86 code.  This can be used to develop web applications with performance of
%native applications. Their implementation consists of two parts --- inner
%sandbox which uses lightweight static analysis do detect security defects and
%outer sandbox which mediates system calls. The implementation has been further
%improved in~\cite{sehr2010} with support for ARM and x86-64 architectures and
%recently has been extended in~\cite{ansel2011} by adding support for
%Just-In-Time (JIT) compilation sandboxing. Again, the disadvantage here is the
%need for modifications of existing code and the use of modified GNU compiler
%toolchain to generate compliant binaries.

%\subsection{Virtualization and Fault-tolerant Computing}

%\begin{structure*}
  %\item Multiscale not Multicore: Efficient Heterogeneous Cloud Computing
  %\item Opportunistic Computing: A New Paradigm for Scalable Realism on Many-Cores
  %\item The Utility Coprocessor: Massively Parallel Computation from the Coffee Shop
%\end{structure*}

%Hardware-level virtualization has already been widely adopted in the industry
%for a variety of purposes.  Companies such as VMware or Microsoft provide a
%wide range of virtualization products, while several high-quality open-source
%solutions such as Xen~\cite{xen} also exist.
  
%Operating system-level virtualization is a method which allows to virtualize
%the operating system kernel into multiple isolated partitions (\ie user-space
%instances).  This type of virtualization is provided for different operating
%systems by products such as FreeBSD Jails~\cite{jails}, Linux
%VServer~\cite{vserver} or Solaris Containers~\cite{containers}. Even though
%these solutions are not as widely deployed as those for hardware-level
%virtualization, they are readily available, have lower overhead, and can be
%employed in many real-world scenarios.
  
%We aim to use \emph{application-level virtualization}, which is a lightweight
%variant of operating system-level virtualization, in which applications run in
%independent execution environments.  Even though this area has not been very
%intensively studied, many of its concepts, such as sandboxing, are becoming
%more and more common, especially in the cloud environment.   Some of these
%features are provided by the products such as VMware ThinApp~\cite{thinapp} or
%Microsoft App-V~\cite{appv}.

%Related research utilizing the concepts of application-level virtualization
%has been described in~\cite{yu2006} proposing lightweight virtual machine
%monitor for virtualization of Windows applications. The authors used namespace
%virtualization, implemented at system call level, to achieve isolation of
%resources between individual applications as well host operating system. This
%work has been followed up in~\cite{yu2008} showing three complete application
%benefiting from the approach presented in the original paper.

%More heavyweight, but also more powerful approach was taken by Dike et al.
%in~\cite{dike2001}. Using system call virtualization (via \texttt{ptrace}
%interface) and device abstraction, they managed to port Linux kernel to
%userspace. The resulting implementation runs a Linux virtual machine in a set
%of processes on a Linux host allowing various applications ranging from kernel
%development to application sandboxing and virtualization. Now, this
%implementation has become an optional module of Linux kernel.

%Virtualization approach is also often used in environments with strong
%reliability requirements, especially in the domain of cluster computing. We
%believe that our approach could be used also in these environments. Shown by
%Schroeder et al.  in~\cite{schroeder2007}, around 20\% of all failures at Los
%Alamos National Laboratory are caused by software failures making them a
%second largest contributor after hardware failures. While today's high
%performance computing systems relies mostly on checkpoint-restart schemes to
%achieve fault tolerance, such as the one proposed in~\cite{srinivasan2004}
%further used in~\cite{qin2005}. This approach is often not sufficient, mainly
%due to large decrease in the effective application utilization. Our approach,
%based on multi-version execution inspired by application-level virtualization
%idea, might present a viable alternative to these approaches.

%\begin{structure}
%\item Control-flow integrity
  %\begin{structure*}
  %\item Control-Flow Integrity
  %\item XFI: Software Guards for System Address Spaces
  %\end{structure*}
%\item Implementation related
  %\begin{structure*}
  %\item Rapid File System Development Using ptrace
  %\item Detours: Binary Interception of Win32 Functions
  %\end{structure*}
%\item Checkpointing and execution rollback
  %\begin{structure*}
  %\item Flashback: A Lightweight Extension for Rollback and Deterministic Replay for Software Debugging
  %\item Rx: Treating Bugs As Allergies
  %\item Making Applications Mobile Under Linux
  %\end{structure*}
%\end{structure}
