\chapter{Conclusion}
\label{chap:conclusion}

Software updates are an integral part of the software development and
maintenance process, but unfortunately they present a high risk, as new
releases often introduce new bugs and security vulnerabilities.

Software updates are an integral part of the software development and
maintenance process. Unfortunately, they also present a high failure risk, and
many users refuse to upgrade their software, relying instead on outdated
versions, which often leave them exposed to known software bugs and security
vulnerabilities.

In this thesis we have proposed a novel multi-version execution approach, a
variant of $N$-version execution, for improving the software update process.
Whenever a new program update becomes available, instead of upgrading the
software to the newest version, we run the new version in parallel with the old
one, and carefully synchronise their execution to create a more reliable
multi-version application.

We have also shown two different approaches for implementing the multi-version
execution approach: \varan, focused on running large number of versions in
parallel with minimal performance overhead; and \mx, focused on recovering from
crashes caused by the faulty software updates.

\varan combines selective binary rewriting with high-performance event
streaming to significantly reduce performance overhead, without sacrificing the
size of the trusted computing base, nor flexibility or ease of debugging.  Our
experimental evaluation has demonstrated that \varan can run C10k network
servers with low performance overhead, and can be used in various scenarios
such as transparent failover and live sanitisation.

\mx uses static binary analysis, system call interposition, lightweight
checkpointing and runtime state manipulation to implement a novel fault recovery
mechanism, which allows for recovery of the crashing version using the code
of the other, non-crashing version. We have shown how \mx can be applied
successfully to several real applications, and recover from real crashes
reported by their users.

\section{Future Work}
\label{conclusion:future-work}

There are number of ways in which both systems could be extended in the future.
In general, we would like to increase the flexibility, to expand the range of
versions we could run in parallel. This means both tolerance to a wider set of
changes as well as ability to detect different type of divergences.

While \varan allows execution of large number of versions in parallel, the
number of versions is currently set at start. We would like enhance our
prototype implementation with the ability to dynamically adjust the number of
versions that are run concurrently. This will ensure that multi-version
applications will be able to utilize all available resources (\ie idle
processor time) without affecting the overall system performance during peak
load.

While scaling down the number of versions is straightforward, to scale up we
would an ability to start new versions and allow them to catch up with the
leader. There are different ways in which such support could be implemented. For
applications structured around a central dispatch loop (\eg network servers or
applications with graphical user interface), we could infer the loop headers
(either statically~\cite{DJgraphs,havlak} or dynamically~\cite{sato11}) and
allow the new versions to "catch up" with the leader at the beginning of the
dispatch loop. The same mechanism could be also used to replace the failing
version in the transparent failover mode.

The recovery mechanism in \mx is based on the ability of restarting the crashed
version using a lightweight checkpointing mechanism. In our current
implementation, we only keep the latest checkpoint for performance reasons.
However, this means that \mx cannot recover a fault caused by a change past the
last checkpoint. We could raise this limitation by keeping more checkpoints and
iteratively retry them if we fail to recover using more recent one. There is a
trade-off involved: keeping more checkpoints incurs higher performance overhead
and we would also need to keep the log of all the system calls between the
checkpointed state and the point of failure, akin to \varan's record \& replay
mechanism.

% Finally, we would like to be able to transparently run multi-version
% applications on multiple underlying platforms, ranging from
% multicore processors to large-scale data centers.  This requires the
% ability to span our virtualized environment across multiple logical as
% well as physical nodes.  In particular, we aim to include the
% possibility of executing certain versions of an application remotely,
% to enable scenarios with hundreds or even thousands of application
% versions.

% During execution, we perform regular checkpoints (\eg based on
% \textstt{fork} which uses copy-on-write internally) and we also record
% all the system calls since the last checkpoint.  On recovery, we first
% restore the failing version from the latest checkpoint and then replay
% all recorded system calls to bring the application to a state just
% before the crash.  The frequency of checkpointing involves a
% trade-off: frequent checkpointing incurs a performance overhead, but
% infrequent checkpointing requires more space to store the system call
% logs.  The two variants of MV execution discussed in this proposal
% would correspond to the two extremes: running the two versions in
% parallel and synchronising the execution at every system call
% (parallel MV execution), and executing the two versions sequentially
% with the second version only being run when the first version crashes
% (sequential MV execution).

% With checkpointing, we can explore points in-between the two extremes,
% and control the different trade-offs more precisely by adaptively
% changing the frequency with which the checkpoints are taken.  One
% strategy would be to monitor the rate of system calls issued by an
% application and adapt the checkpointing rate accordingly.  Ideally, one
% would checkpoint frequently enough to have the log fit in memory, which
% could significantly reduce the recording overhead by eliminating
% expensive I/O operations.

% Furthermore, the way we employ the record and replay mechanism gives us
% additional control over the recovery process.  We can only start replaying the
% execution after the version being recorded crashes, or we can reduce the
% recovery time by running both versions in parallel, recording system calls to a
% log in one version and immediately replaying them in the second version (which
% potentially could be run at lower priority).  While the former variant has been
% used in other contexts, the latter one is novel and more suitable for
% multi-version execution.  Furthermore, by restricting the maximal/minimal
% length of the system call log we can explore the design space in-between these
% two variants.
