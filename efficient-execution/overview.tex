\section{Overview}
\label{sec:overview}

Two key aspects influence the performance and flexibility of an NVX
system: system call interception and version coordination.  We discuss
each in turn below.

\subsection{System call interception}
\label{sec:interception}

%% \begin{enumerate}[(i)]
%% \item wait for the interrupt signaling the entry to a system call;
%% \item examine the registers to determine whether the system
%% call is of interest;
%% \item for any arguments passed by reference, copy the content of
%% the memory for the process address space if necessary;
%% \item if the system call is to be skipped (or performed by the monitor
%% on behalf of the application), replace the original system call with a
%% "null" system call (\ie \lstinline`getpid`);
%% \item restart the execution of the application to execute the system call;
%% \item wait for the interrupt signaling the exit from a system call;
%% \item obtain the process registers to read the system call return value;
%% \item for any output arguments, copy the referenced data from
%% the process address space if needed; and
%% \item continue the execution of the application.
%% \end{enumerate}

The biggest downside of existing system call monitors based on the
\ptrace interface is the high performance
overhead~\cite{orchestra09,tachyon12}.  For each system call
performed by each version, execution must switch to the monitor
process, which has to perform several additional system calls in order
%to be notified about system call entry and exit, 
to copy buffers to and from the version being monitored, nullify the
system call, \etc as shown earlier in
Figure~\ref{fig:lockstep-execution}.

For CPU-intensive applications which perform few system calls, this
overhead will be amortised, translating into a modest overall
slowdown.  However, for heavily I/O-bound applications, the slowdown
can be up to two orders of magnitude, which is unacceptable for many
real-world deployments.
%
Consequently, in order to implement a system call monitor with
acceptable overhead even for heavily I/O-bound applications, we need
to eliminate context switching to the monitor and back during
interception and eliminate the need for additional system calls.  
This is accomplished through a combination of selective binary
rewriting and an interprocess communication mechanism based on a
fast shared memory ring buffer.

%\vspace{0.1in} \noindent \textbf{Selective binary rewriting.}  
Whenever code is loaded into memory, \varan scans each code page to
selectively rewrite all system calls with jump instructions to dedicated
handlers.  Section \ref{sec:rewriting} discusses in detail the main
steps and challenges associated with this binary rewriting approach.

To eliminate the need for additional system calls during interception,
\varan uses a shared ring buffer to communicate between versions.  This
ring buffer is heavily optimised for performance: it is stored in
memory, allows largely lock-free communication, and does not require
the dispatch of events to different queues.  These aspects are
discussed in detail in Section~\ref{sec:streaming}.



\subsection{Event-streaming architecture}
\label{sec:coordination}

In prior NVX systems, versions are typically run in lockstep, with a
centralised monitor coordinating and virtualising their execution.
Essentially, at each system call, the versions pass control to the
monitor, which waits until all versions reach the same system call.
Once this happens, the monitor executes the system call and
communicates the result to each individual version.  If two or more
versions try to break the lockstep by executing different system
calls, the monitor needs to either terminate the entire application or
continue executing a subset of the versions in lockstep.

This approach has two key disadvantages.  First, the centralised
monitor is a bottleneck, which can have a significant impact on
performance.  Note that in addition to the synchronisation overhead,
this centralised monitor makes the NVX application execute at the
speed of the slowest individual version.

Second, this approach is totally inflexible to any divergence in the
sequence of system calls executed across versions.  This is an issue
both when running automatically-diversified variants, where certain
transformations may affect the external behaviour, and when running
existing software revisions, where changes in the sequences of
system calls can occur between revisions. 

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{efficient-execution/figures/architecture}
    \caption{The event-streaming architecture of \varan.}
    \label{fig:architecture}
  \end{center}
\end{figure}

To address these limitations, \varan uses a new approach which we call
\emph{event streaming}.  In this decentralised architecture,
depicted in Figure~\ref{fig:architecture}, one of the
versions is designated as the \textit{leader}, while the others are
\textit{followers}. During execution, the leader records all events into a
shared ring buffer, which are later read by followers to mimic the leader's
external behaviour (\S\ref{sec:streaming}). Events consist primarily
of regular system
call invocations, but also of signals, process forks (\ie \lstinline`clone`
and \lstinline`fork` system calls) and exits (\ie \lstinline`exit` and
\lstinline`exit_group` system calls).

In general, any version can be the leader, although in some situations
some may be a better choice than others, \eg when running multiple
software revisions in parallel, one might prefer to designate the
newest one as leader.  However, the leader can be easily replaced if
necessary, \eg if it crashes (\S\ref{sec:leader-repl}).

The only centralised component in this architecture is the
\textit{coordinator}, whose main job is to prepare the versions for
execution and establish the necessary communication channels.  At a
high level, the coordinator first loads the variants into memory,
injects several special handlers and memory objects into their address
spaces, rewrites any system calls in their code with jumps to the
special handlers and then starts executing the variants
(\S\ref{sec:setup}) in a decentralised manner.

%% recorded by one application version are shortly replayed
%% (\textit{streamed}) to the others, which keeps the log small, as
%% events which have been replayed by all versions can be discarded.
%% Similarly, the NVX context allows for the log to be kept in memory,
%% and for the replay to be done incrementally, with significant
%% performance advantages.  Event streaming is discussed in detail in
%% Section~\ref{sec:streaming}.


%% This is a variant of record-replay~\cite{scribe,jockey,geels06,r2},
%% but the NVX context allows us to overcome two of the main limitations
%% of traditional record-replay techniques, namely (1)~the
%% rapidly-growing log size, especially for system call-intensive
%% applications; and (2)~the long time necessary to replay the execution.
%% Because the multiple versions are executed concurrently, events
%% recorded by one application version are shortly replayed
%% (\textit{streamed}) to the others, which keeps the log small, as
%% events which have been replayed by all versions can be discarded.
%% Similarly, the NVX context allows for the log to be kept in memory,
%% and for the replay to be done incrementally, with significant
%% performance advantages.  Event streaming is discussed in detail in
%% Section~\ref{sec:streaming}.


\subsection{Rewrite rules for system call sequences}
\label{sec:rw}

In addition to eliminating the central monitor bottleneck, our
event-streaming architecture also supports (small) divergences between
the system call sequences of different variants.  For example,
different software revisions can be run inside a classical NVX
system only as long as they all issue the same sequence of system
calls~\cite{mx}.  However, software patches sometimes change the
external behavior of an application.  In particular, many divergences
in system call traces fall into the following two categories:
\begin{inparaenum}[(i)]
\item \emph{addition/removal}, characterising situations when one of
  the versions performs (or conversely does not perform) an additional
  system call, typically as a consequence of an additional check, and
\item \emph{coalescing}, covering the situations when a (repeated)
  sequence of system calls is executed a different number of times in
  each version (\eg one version might execute two \lstinline`write`
  system calls, while another version executes only one
  \lstinline`write` system call to write the same bytes because extra
  buffering is used).

\end{inparaenum}

% \begin{description}
%   \item[Addition/Removal] This class characterises situations when one
%     of the versions performs an additional system call (or conversely
%     does not perform), typically as a consequence of an additional
%     check.
%   \item[Coalescing] This class covers the situations when a (repeated)
%     sequence of system calls is executed a different number of times
%     in each version.  E.g., one version might execute two \lstinline`write`
%     system calls, while another a single equivalent \lstinline`write` to
%     write the same bytes (\eg because extra buffering is used).  
% \end{description}

\varan is the first NVX system that is able to deal with such changes.
When followers process the event sequence streamed by the leader, they
can rewrite it to account for any such differences: \eg they can skip
and merge system calls, or perform some calls themselves.  We provide
a flexible implementation of such rewrite rules using Berkeley Packet
Filters (\S\ref{sec:patternmatching}).

