\chapter{Multi-version Execution}
\label{chap:multi-version}

An indisputable and widely accepted fact is that all software contains bugs.
These bugs cause software failures which can lead to anything from minor
discomfort and loss of service to major disasters including loss of lives.
Software reliability is the probability of failure-free operation for a
specified period of time in a specified environment.

There are two basic approaches to increase software reliability. One is fault
avoidance, using formal specification and verification methods, and a rigorous
software development process. Another approach is fault tolerance, through
replication, redundancy and diversity. One of the popular software fault
tolerance methods that use diversity is $N$-version
programming~\cite{avizienis:nvp}.

%$N$-version programming (NVP) has been first introduced in 1970's as a way to
%increase reliability of software by having independent teams to design and
%implement the same functionality multiple times from the same specification as
%different program versions. 

The idea behind $N$-version programming (NVP) is to have independent teams
design and implement the same functionality multiple times from the same
specification as different program versions. These versions are then run in
parallel using an $N$-version execution environment (NVX) and their results are
voted on and the majority of the outputs are selected.

NVP depends on the assumption that software errors in different versions are
independent. Otherwise, these versions are not effective at detecting errors as
the different versions are likely to contain the same bugs. The experiment done
by Knight and Leveson~\cite{knight86,knight90} showed that the assumption of
independence does not hold and independently developed versions had the same or
similar software faults 50\% of the time since programmers tended to commit
certain classes of mistakes independently.

The problem of NVP is that even a small probability of correlated bugs in
different versions significantly reduces the potential reliability improvement,
which makes NVP less cost-effective compared to other fault-tolerance
techniques due to the need for developing $N$ different versions. Lui
Sha~\cite{lui01} compared different fault tolerance approaches and suggested
that, assuming a finite software development cycle, it is better to invest
resources into a single reliable implementation rather than three different
implementations which will be run in parallel.

The issue of cost related to the development of multiple independent versions
has been partially addressed by the multi-variant program execution research,
which can be seen as an evolution of $N$-version programming. Rather than
developing different versions of the same application manually, these are
generated from a single version using different techniques such as special
source code annotations~\cite{onlinevalidation,trachsel10}, code
transformations based on the modification of the abstract syntax
tree~\cite{schulte14,sosie:issta14}, custom compilers producing different
binary layouts for different variants~\cite{orchestra09,unibus:nspw10}.

We could also run different programs where one program acts as a specification
and other implementations are checked against the specification at runtime.
This is the basic idea behind runtime monitoring and
checking~\cite{kim:mac,java-mac01}. In runtime monitoring and checking, one
program is the implementation while another is the specification. The
implementation is checked against the specification at runtime, which is easier
than proving the equivalence statically (or formally).  Furthermore, the
specification can be less efficient and thus simple. The example shown by Lui
Sha in~\cite{lui01} uses Bubblesort as a runtime specification for Quicksort.

%However, Hatton~\cite{hatton97} argued that even dependent $N$-version
%programming provides significant reliability improvement and it is therefore
%more cost-effective to develop $N$ average versions rather than one good
%version.

%\section{Software updates}

% Recent years have seen a growing interest in using diversity as a way to
% increase the reliability and security of software systems.  One form of
% software diversity that has attracted significant interest from the research
% community is the idea of running multiple diversified versions of a program in
% parallel in order to survive bugs and detect security attacks.  In essence,
% diversity can offer probabilistic guarantees that at least one variant survives
% a bug, or that a security attack will be flagged by divergent behaviour across
% variants.

% On the reliability side, which forms the main focus of this thesis, these
% diversified versions are either automatically-generated variants, multiple
% versions of the same application, or different programs implementing the same
% interface.  For example, one may run in parallel multiple variants that employ
% complementary thread schedules to survive concurrency
% errors~\cite{compl-schedules11}, multiple versions of the same software to
% survive update bugs~\cite{mx}, or multiple web browsers to benefit from the
% fact that many errors do not affect all browser
% implementations~\cite{cocktail}.  In this thesis, we show that running multiple
% versions in parallel can be used in other reliability scenarios, such as
% running expensive error detectors (``sanitizers'') during deployment.

% On the security side, these diversified variants are constructed in such a way
% as to reduce the probability of an attack succeeding in all of
% them~\cite{cox2006,orchestra09,diehard06,tightlip,capizzi08,devries10,cocktail,trachsel10}.
% For example, one may generate versions with a different arrangement of memory
% blocks in the address space~\cite{diehard06}, or with stacks growing in
% opposite directions~\cite{orchestra09}, to prevent attacks whose success
% depends on the memory layout.

One particularly important class of bugs are bugs introduced by software
updates. These bugs often affect existing functionality, degrading the
reliability, and potentially also security, of the software. This leads to
frustration as users are no longer able to use the functionality they have been
able to use in the past; as a consequence, users often refuse applying updates
to their software and rely instead on outdated versions, which may contain bugs
and security vulnerabilities.

To address this problem, we propose a variant of NVX focused on improving
reliability and availability of evolving software: \emph{multi-version
execution}.  While NVX and multi-version execution techniques have many
similarities, there are several key differences.  First and foremost, in NVX
the different versions are developed by different teams while multi-version
execution uses the subsequent revisions, which are the product of natural
software evolution. Second, the goal of NVX is to increase the reliability of
software by running independently developed, functionally equivalent software
variants, whereas multi-version execution assumes that the variants are largely
the same except for the parts affected by the software update.

While designed primarily in the context of software updates, the multi-version
execution monitors and environments can benefit NVX too. Furthermore, as shown
in this thesis, multi-version execution runtimes can be used in other
reliability scenarios, such as running expensive error detectors
(``sanitizers'') during deployment or implementing security honeypots.

% There are two key differences between our proposed approach and
% previous work in this space.  First, we do not rely on automatically
% generated variants, but instead propose to use existing software
% versions as a mechanism for improving software updates.  This also
% means that as opposed to previous solutions, the versions running in
% parallel are not semantically equivalent---this eliminates the
% challenge of generating diversified variants and creates opportunities
% in terms of recovery from failures, but also introduces additional
% challenges in terms of synchronising the execution of the different
% versions.  Second, while previous work has focused on detecting
% divergences, our key concern is to \textit{survive} them,
% in order to increase the reliability, availability, and security 
% of the overall application.

% Running different versions of an application in parallel has been used
% to test and validate software patches.  Tachyon~\cite{tachyon12} is an
% online patch testing system in which the old and the new version of an
% application are run concurrently; 
% when a divergence is detected, the options are to either halt the
% program, or to create a manual rewrite rule specifying how to handle
% the divergence.  Delta execution~\cite{onlinevalidation} similarly
% uses two different versions of a single application, splitting the
% execution at points where the two versions differ, and comparing their
% results to test the patch for errors and validate its
% functionality.  
% By comparison, the focus of this proposal is on ``managing'' such
% divergences at runtime in order to keep the application running, and
% therefore runtime deployment and crash recovery play a central role in
% our approach.

% Research on surviving software failures has received a lot of
% attention in the
% past~\cite{rx,compl-schedules11,fo,exec-trans06,vigilante,clearview,microreboots},
% and our proposed approach can benefit from the techniques developed in
% this context.  Compared to this prior work, the main novelty lies in
% combining multiple software versions to survive failures.

\input{multi-version/scenarios}
\input{multi-version/rationale}
\input{multi-version/scope}
\input{multi-version/summary}
