\chapter{Multi-version Execution}
\label{chap:multi-version}

An indisputable fact is that software contains bugs. These bugs cause software
failures which can lead to anything from minor discomfort and loss of service
to major disasters including loss of lives. Software reliability is the
probability of failure-free operation for specified period of time in a
specified environment.

There are two basic approaches to increase software reliability. One is fault
avoidance, using formal specification and verification methods, and a rigorous
software development process. Another approach is fault tolerance, through
replication, redundancy and diversity. Two popular software fault tolerance
methods that use diversity are $N$-version programming~\cite{avizienis:nvp} and
recovery block~\cite{randell:rb}.

%$N$-version programming (NVP) has been first introduced in 1970's as a way to
%increase reliability of software by having independent teams to design and
%implement the same functionality multiple times from the same specification as
%different program versions. 

The idea behind $N$-version programming (NVP) is to have independent teams to
design and implement the same functionality multiple times from the same
specification as different program versions. These versions are then run in
parallel using an $N$-version execution environment (NVX) and their results are
voted on and the majority of the outputs is selected.

$N$-version programming depends on the assumption that software errors in
different versions are independent. Otherwise, these versions are not effective
at detecting errors as the different versions are likely to contain the same
bugs. The experiment done by Knight and Leveson~\cite{knight86,knight90} shown
that the assumption of independence does not hold and independently developed
versions had the same or similar software faults 50\% of the time in more than
one version since programmers tended to commit certain classes of mistakes
dependently.

Despite the criticism, $N$-version programming is still widely accepted as an
effective technique for improving reliability. The problem is that even small
probability of correlated bugs in different version significantly reduces the
potential reliability improvement, which makes the $N$-version programming less
cost-effective compared to other fault-tolerance due to a need for developing
$N$ different version. Lui Sha~\cite{lui01} compared different fault tolerance
approaches and suggested that, assuming finite software development cycle, it
is better to invest resources into a single reliable implementation rather than
three different implementations which will be run in parallel.

The issue of cost related to development of multiple independent versions has
been partially addressed by the multi-variant program execution, which can be seen
as an evolution of $N$-version programming. Rather than developing different versions
of the same application manually, these are generated from a single version
using different techniques such as special source code
annotations~\cite{onlinevalidation,trachsel10}, code transformations based on
the modification of the abstract syntax tree~\cite{schulte14,sosie:issta14},
custom compilers producing different binary layout for different
variants~\cite{orchestra09,unibus:nspw10}. %, or even specialized runtime
%environments which customize different aspects of application runtime
%state~\cite{diehard06,tightlip}.

Rather than executing $N$ different versions of the same program, we could also
run different programs where one program acts as a specification and other
implementations are checked against the specification at runtime.  This is the
basic idea behind runtime monitoring and checking~\cite{kim:mac,java-mac01}. In
runtime monitoring and checking, one program is the implementation while
another is the specification. The implementation is checked against the
specification at runtime, which is easier than proving the equivalence
statically (or formally).  Furthemore, the specification can be less efficient
and thus simple. The example shown by Lui Sha in~\cite{lui01} uses Bubblesort
as a runtime specification for Quicksort.

%However, Hatton~\cite{hatton97} argued that even dependent $N$-version
%programming provides significant reliability improvement and it is therefore
%more cost-effective to develop $N$ average versions rather than one good
%version.

\section{N-version \vs multi-version execution}

% Recent years have seen a growing interest in using diversity as a way to
% increase the reliability and security of software systems.  One form of
% software diversity that has attracted significant interest from the research
% community is the idea of running multiple diversified versions of a program in
% parallel in order to survive bugs and detect security attacks.  In essence,
% diversity can offer probabilistic guarantees that at least one variant survives
% a bug, or that a security attack will be flagged by divergent behaviour across
% variants.

% On the reliability side, which forms the main focus of this thesis, these
% diversified versions are either automatically-generated variants, multiple
% versions of the same application, or different programs implementing the same
% interface.  For example, one may run in parallel multiple variants that employ
% complementary thread schedules to survive concurrency
% errors~\cite{compl-schedules11}, multiple versions of the same software to
% survive update bugs~\cite{mx}, or multiple web browsers to benefit from the
% fact that many errors do not affect all browser
% implementations~\cite{cocktail}.  In this thesis, we show that running multiple
% versions in parallel can be used in other reliability scenarios, such as
% running expensive error detectors (``sanitizers'') during deployment.

% On the security side, these diversified variants are constructed in such a way
% as to reduce the probability of an attack succeeding in all of
% them~\cite{cox2006,orchestra09,diehard06,tightlip,capizzi08,devries10,cocktail,trachsel10}.
% For example, one may generate versions with a different arrangement of memory
% blocks in the address space~\cite{diehard06}, or with stacks growing in
% opposite directions~\cite{orchestra09}, to prevent attacks whose success
% depends on the memory layout.

While $N$-version execution and multi-version execution techniques have many
similarities, there are several key differences. First and foremost, in
$N$-version execution the different version are developed by different teams
while multi-version execution uses the consequent revisions, which are product
of natural software evolution. Second, the goal of $N$-version execution is
increasing the reliability of software by running independently developed,
functionally equivalent software variants. While multi-version execution can be
applied in many different scenarios, including $N$-version execution, the
technique was designed to in the context of software updates. Furthermore, as
shown in this thesis (\S\ref{chap:applications}), running multiple versions in
parallel can be used in other reliability scenarios, such as running expensive
error detectors (``sanitizers'') during deployment or implementation of
security honeypots.

% There are two key differences between our proposed approach and
% previous work in this space.  First, we do not rely on automatically
% generated variants, but instead propose to use existing software
% versions as a mechanism for improving software updates.  This also
% means that as opposed to previous solutions, the versions running in
% parallel are not semantically equivalent---this eliminates the
% challenge of generating diversified variants and creates opportunities
% in terms of recovery from failures, but also introduces additional
% challenges in terms of synchronising the execution of the different
% versions.  Second, while previous work has focused on detecting
% divergences, our key concern is to \textit{survive} them,
% in order to increase the reliability, availability, and security 
% of the overall application.

% Running different versions of an application in parallel has been used
% to test and validate software patches.  Tachyon~\cite{tachyon12} is an
% online patch testing system in which the old and the new version of an
% application are run concurrently; 
% when a divergence is detected, the options are to either halt the
% program, or to create a manual rewrite rule specifying how to handle
% the divergence.  Delta execution~\cite{onlinevalidation} similarly
% uses two different versions of a single application, splitting the
% execution at points where the two versions differ, and comparing their
% results to test the patch for errors and validate its
% functionality.  
% By comparison, the focus of this proposal is on ``managing'' such
% divergences at runtime in order to keep the application running, and
% therefore runtime deployment and crash recovery play a central role in
% our approach.

% Research on surviving software failures has received a lot of
% attention in the
% past~\cite{rx,compl-schedules11,fo,exec-trans06,vigilante,clearview,microreboots},
% and our proposed approach can benefit from the techniques developed in
% this context.  Compared to this prior work, the main novelty lies in
% combining multiple software versions to survive failures.

\input{multi-version/scenarios}

\section{Parallel software execution}

The bottom line for all the scenarios presented hereinbefore is that for
significant amount of time, \ie about one year in case of \lighttpd and six
months in case of \redis, users affected by buggy patches essentially had
to decide between%
\begin{inparaenum}[(1)]
\item incorporating other security and bug fixes added to the code, but being
  vulnerable to these crash bug, and
\item giving up on these security and bug fixes and using an old version of
  \lighttpd or \redis, which are not vulnerable to these newly introduced bugs.
\end{inparaenum}
Note that this is particularly true for the period between the time when the
bug was introduced and the time it was diagnosed (\ie eleven months in case of
\lighttpd, six months in case of \redis), since during this period most users
would not know how to change the server's configuration to avoid the crash.

Our approach aims to provide users with a third choice; when a new version
arrives, instead of replacing the old version, we will run both. The goal
of our approach is to run multiple versions of an application in parallel, and
synchronize their executions so that%
\begin{inparaenum}[(1)]
\item users are given the illusion that they interact with a single version of
  the application,
\item the multi-version application is at least as reliable and secure as any
  of the individual versions in isolation, and
\item the synchronization mechanism incurs a reasonable performance overhead.
\end{inparaenum}

As shown in Figure~\ref{fig:mx-platform}, our solution requires a form of
virtualization framework that would coordinate the execution of multiple
application versions, and mediate their interaction with the external
environment. Various such frameworks have been designed in the past in the
context of running multiple automatically generated variants of a given
application in parallel~\cite{diehard06,cox2006,orchestra09}, and many of the
techniques proposed in prior work can be reused in our context.  To be
practical, this coordination mechanism has to incur a reasonable overhead on
top of native execution and ensure that the overall system is able to scale up
and down the number of software versions run in parallel in order to balance
conflicting requirements such as performance, reliability, security and energy
consumption.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{multi-version/figures/platform}
    \caption{A platform running conventional and multi-version
      applications side by side.}
    \label{fig:mx-platform}
  \end{center}
\end{figure}

One particular challenge for our approach is to detect any divergences between
different software versions, resolve them in such a way as to increase the
overall reliability of the application, and finally synchronize again the
different versions after their executions reconverge to the same behavior.  Of
course, we also need to handle the case in which the executions of different
versions fail to reconverge to the same behavior after sufficient time.

Selecting the ``correct'' behavior of an application when different versions
disagree is of course not possible in the general case without having access to
a high-level specification.  However, one could%
\begin{inparaenum}[(1)]
\item focus on universal
correctness properties, such as the absence of crashes, and
\item use various
heuristics such as majority voting and favoring the latest application
versions.
\end{inparaenum}
Our approach is to resolve a divergence by always using the behavior of the
version that has not crashed, and favoring the behavior of the latest version
in all other cases. In this way, we ensure that the overall application has
strictly fewer crashes than any of the individual versions, while still using
the new security and bug fixes implemented in the latest version.

% Note that one key aspect on which our approach relies is
% \textit{having the different versions be alive at all times}.  This ensures
% that applications can survive crashes that occur at different points
% in different versions, but adds the extra challenge of restarting
% crashed versions.

%% Once divergences are detected, we need to decide what output should be
%% selected as the output of the multi-version system in order to improve
%% overall reliability.  Many different strategies can be used, but the
%% common goal is to ensure that the multi-version system will have
%% strictly fewer errors than any of the individual versions.

% Finally, our approach needs a deployment strategy to decide what versions are
% run in parallel when the number of available resources (\eg, idle CPU cores) is
% limited.  We envision several options---such as keeping the last $N$ released
% versions (where $N$ is the number of available resources), or keeping several
% very old stable versions---but the exact strategy should be decided on a
% case-by-case basis. % using existing empirical data.

In this thesis, we present two different designs for building such monitors;
one, called \varan, aimed at running large number of versions side-by-side with
low overhead, and second, called \mx, focused on surviving crashes caused by
bugs introduced in software updates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our approach aims to provide users with a third choice; when a new version
arrives, instead of replacing the old version, we run both versions in
parallel. In our example, consider that we are using \mx to run a
version of \lighttpd from March 2009.  When the buggy April 2010 version
is released, \mx runs it in parallel with the old one.  As the two
versions execute:

\begin{itemize}
\item As long as the two versions have the same external behaviour (\eg they
  write the same values into the same files, or send the same data over the
  network), they are run side-by-side and \mx ensures that they act as one to
  the outside world;

\item When one of the versions crashes (\eg the new version executes the buggy
  patch), \mx will patch the crashing version at runtime using the behaviour of
  the non-crashing version.  In this way, \mx can successfully survive crash
  bugs in both the old and the new version, increasing the reliability and
  availability of the overall application;

\item When a non-crashing divergence is detected, \mx will discard one of the
  versions (by default the old one, but other heuristics can be used).  The
  other version can be later restarted at a convenient synchronisation point
  (\eg at the beginning of the dispatch loop of a network server).
\end{itemize}

To enable these scenarios, a monitor process coordinates the parallel execution
of these variants\footnote{The terms \textit{version} and \textit{variant} are
used interchangeably.} and synchronises their execution, making them appear as
a single application to any outside entities.  While synchronisation can be
performed at different levels, the most common approach is to do it at the
level of system calls, for two main reasons: first, many existing
diversification transformations, such as address-space layout
randomisation~\cite{diehard06} and instruction-set
randomisation~\cite{instr-set-rand03} do not change the sequence of system
calls (the program's \textit{external behaviour}), and the ordering is often
preserved even across different software versions.  Second, system
calls are the main way in which the application communicates with the outside
environment, and therefore
%% the ultimate target of attackers.  Finally, as the main
%% communication mechanism between applications and the environment,
%% system calls
must be virtualised in order to enable the multiple versions to act as
one to the outside world.

There are many challenges in impleementing such monitor. \todo{List a couple of them...}
\begin{structure}
\item Trade-off between performance, security, flexibility, ease-of-debugging.
\item Access to source code, instrumentation vs running on binaries.
\end{structure}
