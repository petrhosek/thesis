\chapter{Multi-version Execution}
\label{chap:multi-version}

An indisputable fact is that software contains bugs. These bugs cause software
failures which can lead to anything from minor discomfort and loss of service
to major disasters including loss of lives. Software reliability is the
probability of failure-free operation for specified period of time in a
specified environment.

There are two basic approaches to increase software reliability. One is fault
avoidance, using formal specification and verification methods, and a rigorous
software development process. Another approach is fault tolerance, through
replication, redundancy and diversity. Two popular software fault tolerance
methods that use diversity are $N$-version programming~\cite{avizienis:nvp} and
recovery block~\cite{randell:rb}.

%$N$-version programming (NVP) has been first introduced in 1970's as a way to
%increase reliability of software by having independent teams to design and
%implement the same functionality multiple times from the same specification as
%different program versions. 

The idea behind $N$-version programming (NVP) is to have independent teams to
design and implement the same functionality multiple times from the same
specification as different program versions. These versions are then run in
parallel using an $N$-version execution environment (NVX) and their results are
voted on and the majority of the outputs is selected.

$N$-version programming depends on the assumption that software errors in
different versions are independent. Otherwise, these versions are not effective
at detecting errors as the different versions are likely to contain the same
bugs. The experiment done by Knight and Leveson~\cite{knight86,knight90} shown
that the assumption of independence does not hold and independently developed
versions had the same or similar software faults 50\% of the time in more than
one version since programmers tended to commit certain classes of mistakes
dependently.

Despite the criticism, $N$-version programming is still widely accepted as an
effective technique for improving reliability. The problem is that even small
probability of correlated bugs in different version significantly reduces the
potential reliability improvement, which makes the $N$-version programming less
cost-effective compared to other fault-tolerance due to a need for developing
$N$ different version. Lui Sha~\cite{lui01} compared different fault tolerance
approaches and suggested that, assuming finite software development cycle, it
is better to invest resources into a single reliable implementation rather than
three different implementations which will be run in parallel.

The issue of cost related to development of multiple independent versions has
been partially addressed by multi-variant program execution, which can be seen
as an evolution of $N$-version programming. Rather developing different versions
of the same application manually, these are generated from a single version
using different techniques such as \ldots.

Rather than executing $N$ different versions of the same program, we could also
run different programs where one program acts as a specification and other
implementations are checked against the specification at runtime.  This is the
basic idea behind runtime monitoring and checking \todo{citation?}. In runtime
monitoring and checking, one program is the implementation while another is the
specification. The implementation is checked against the specification at
runtime, which is easier than proving the equivalence statically (or formally).
Furthemore, the specification can be less efficient and thus simple. The example
shown by Lui Sha in~\cite{lui01} uses Bubblesort as a runtime specification for
Quicksort.

%However, Hatton~\cite{hatton97} argued that even dependent $N$-version
%programming provides significant reliability improvement and it is therefore
%more cost-effective to develop $N$ average versions rather than one good
%version.

\section{N-version \vs multi-version execution}

While $N$-version execution and multi-version execution techniques have many
similarities, there are several key differences.

First and foremost, in $N$-version execution the different version are developed
by different teams while multi-version execution uses the consequent revisions,
which are product of natural software evolution.

\section{Two applications of multi-version execution}

While multi-version execution can be applied in many different scenarios, as
further shown in Chapter~\ref{chap:applications}, the technique was designed to
in the context of software updates.  

\subsection{Fail recovery for updated software}
\subsection{Parallel execution of software versions}
%\input{nversion-updates/prototype}

\section{Safe software updates}

\section{Parallel software execution}

\section{Scenarios}
\label{multi-version:scenarios}

In this section we motivate our approach using existing scenarios involving the
\chrome browser and the \vim editor, as well as the \lighttpd and \vsftpd
servers.  These correspond to two categories of applications that could benefit
from our multi-version software update approach: desktop applications such as
web browsers and office tools for which reliability is a key concern; and
network servers, with stringent security and dependability requirements.

%% We also believe, that the proposed approach might facilitate and
%% improve the
%% \emph{continuous deployment} technique. While this technique has been
%% frequently proposed and advocated in the software engineering community
%% because it encourages experimentation, innovation, and rapid
%% iteration~\cite{johnson2009,harmess2009,linden2009}, its use is still limited
%% since updates may introduce new bugs and security vulnerabilities.  We believe
%% that our approach provides all the benefits of continuous deployment without
%% compromising software reliability.

%\subsection{Google Chrome Web Browser}
%\label{sub:chrome}

\gchrome\footnote{\url{http://www.google.com/chrome}} is one of the most widely
used web browsers.  Even though \chrome releases are tested extensively before
deployment, they sometimes introduce new bugs that affect the stability of the
browser.  A concrete example is version $6.0.466.0$, which introduced a bug
that caused \chrome to crash when trying to load certain web pages over
SSL.\footnote{\url{http://code.google.com/p/chromium/issues/detail?id=49197}}
One might argue that in this case the user should downgrade to an older version
and wait until the bug is fixed. However, versions immediately preceding
$6.0.466.0$ suffer from a different
bug,\footnote{\url{http://code.google.com/p/chromium/issues/detail?id=49721}}
which was introduced in version $6.0.438.0$ and which crashes Chrome during
certain sequences of repeated back and forward navigation.

\vim\footnote{\url{http://www.vim.org/}} is arguably one of the most popular
text editors.  In version $7.1.127$, while trying to fix a memory leak,
\vim developers introduced a double \textstt{free} bug that caused \vim to
crash whenever the user tried to use a path completion feature.  This bug made
its way into \textit{Ubuntu} $8.04$, affecting a large number of
users.\footnote{\url{https://bugs.launchpad.net/ubuntu/+source/vim/+bug/219546}}
%% This bug remained undetected until version $7.1.138$, which has been
%% distributed with Ubuntu $8.04$ one of the most popular Linux
%% distributions.  The bug was caused by a

\lighttpd\footnote{\url{http://www.lighttpd.net/}} is a popular web-server used
by several high-traffic websites such as YouTube, Wikipedia, and Meebo. Despite
its popularity, faulty updates are still a common occurrence in \lighttpd.
%, as evident from its bug tracking database. 
As one example, a patch introduced in April
2009\footnote{\url{http://redmine.lighttpd.net/projects/lighttpd/repository/revisions/2438}}
(correctly) fixed the way HTTP ETags are computed.
%% The patch was a one-line change, which discarded the terminating zero
%% when computing a hash.
Unfortunately, this fix broke the support for compression, which relied on the
previous way in which ETags were computed and resulted in a segmentation fault
whenever a client requested HTTP compression.  This issue was only diagnosed
and reported in March
2010\footnote{\url{http://redmine.lighttpd.net/issues/2169}} and fixed at the
end of April
2010,\footnote{\url{http://redmine.lighttpd.net/projects/lighttpd/repository/revisions/2723}}
more than one year after it was introduced, leaving the server vulnerable to
possible attacks in between.

\vsftpd\footnote{\url{https://security.appspot.com/vsftpd.html}} is a fast and 
secure FTP server for UNIX systems.  Version $2.2.0$ added several new
features such as network isolation, but unfortunately also introduced
a
bug\footnote{\url{https://bugs.launchpad.net/ubuntu/+source/vsftpd/+bug/462749}}
which triggered a segmentation fault whenever a client used the
passive FTP mode.  This bug made \vsftpd practically unusable since
the passive mode is being frequently used by clients behind firewalls.
Despite being reported several times, this bug was only fixed in
version $2.2.1$, released more than two months after the bug was
introduced.
%% This is even more surprising given that the bug was triggered by a
%% missing data structure allocation in the newly introduced
%% configuration processing and the patch for this bug consists of a
%% single line.


All the scenarios presented above describe software updates which,
while trying to add new features or bug fixes, also introduced new
bugs that caused the code to crash under certain conditions.
Improving the reliability of such updates is the main goal of our
proposed approach: running both the old and the new version in
parallel after an update can enable applications to survive more
errors, without giving up the new features introduced by the update.

%\section{N-version Updates}
%\input{background/nversion-updates}

%\section{Safe Software Updates}
%\input{background/safe-updates}

%\section{Efficient Execution}
%\input{background/efficient-execution}

%\subsection{Execution of Multiple Versions}

%The idea of concurrently running multiple versions (or a \emph{multi-version
%execution}) of the same application was first explored in the context of
%$N$-version programming, a software development methodology introduced in the
%seventies in which multiple teams of programmers develop functionally
%equivalent versions of the same program in order to minimize the risk of having
%the same bugs in all versions~\cite{chen1995}. Furthermore, the use of an
%execution environment responsible for running the $N$-version programs and
%choosing one of the outputs was proposed.
  
%This idea has been followed up in~\cite{cox2006}, which proposes the use of
%automatically generated {\it diversified variants} of the same program to
%increase application security. Through the use of multiple variants with
%potentially disjoint exploit sets, the proposed approach makes it difficult to
%exploit a large class of security vulnerabilities such as buffer overflow, as
%attackers would need to simultaneously compromise all variants.

%The idea has been implemented in \cite{orchestra09}, which uses a modified
%compiler to produce two versions of the same application with stacks growing in
%opposite directions, together with an execution environment that monitors both
%variants at runtime, checking for any discrepancies in system calls made by the
%variants that would indicate a successful security attack on one of the
%replicas. This work has been further improved in \cite{orchestra11} with new
%method for signal delivery accompanied with detailed analysis of the benchmark
%characteristics. However, their approach is still limited to two versions
%running in lockstep with only difference being the opposite growing stack.
%Most importantly, whenever divergence between the two versions is detected,
%they are immediately stopped.

%The multi-version execution idea has been also used for different purposes ---
%Berger et al. describe a similar approach that uses address space layout
%randomization to generate multiple replicas that are executed
%concurrently~\cite{diehard06}. This approach is combined with randomization
%and replication to provide memory error tolerance. The goal of their work is to
%increase reliability by tolerating memory errors in exchange for space costs
%and execution time.

%Solution described in~\cite{shye2009} runs multiple instances of the same
%version of a single application aiming to overcome transient failures by
%monitoring and comparing their execution on the level of kernel system calls.
%However, this solution does not aim to overcome such failures.

%Running different versions of an application in parallel has also been used to
%test and validate software patches.  In~\cite{onlinevalidation}, two different
%versions of a single application are run in parallel, splitting the execution
%at points where the two versions differ, and comparing their results to test
%the patch for errors and validate its functionality. Whenever one of the two
%versions crashes, a bug is reported. This approach is limited only to a
%specific categories of patches such as refactoring or changes to rarely used
%paths (\eg error handlers). Moreover, this approach is only targeted towards
%on-line validation, not as a generally usable runtime environment.

%Trachsel et al.  use a similar approach to increase performance, where program
%variants are obtained by using different (compiler) optimizations and
%algorithms~\cite{trachsel10}.  The goal is to increase the overall system
%performance by always selecting the variant which finishes its execution first.
%Thereby, no synchronization across variants is needed.

%\subsection{Multi-version Environment and Update Management}

%Closely related to the execution of multiple versions is the management of
%multiple versions, environment and software updates, such as deciding when to
%upgrade, applying updates, \etc

%Solution, which in some sense resembles the multi-version execution idea, has
%been proposed by Michael Franz in~\cite{franz2010} --- instead of executing
%multiple versions in parallel, he proposes to distribute unique version of
%every program to every user. Such versions should be created automatically by
%a ``multicompiler'' and distributed to users through ``App Store''. This would
%increase security as it would be much more difficult to generate attack
%vectors by reverse engineering of security patches for these diversified
%versions.

%To make such a solution work in practice, the way to manage large number of
%different versions and the overall update process is needed. This idea has
%been explored in~\cite{crameri:updates}. This framework integrates upgrade
%deployment, user-machine testing and problem reporting into the overall
%upgrade process.  The framework itself clusters users' machines according to
%their environment, tests the upgrades using cluster representatives and allows
%deployment of complex upgrades.

%Beattie et al. showed in~\cite{beattie2002} that the timing of security patch
%applying can be of critical importance. Patching too early could result in
%breaking the system by applying broken patch, patching too late could on the
%other hand lead to risk of penetration by an attacker exploiting a well known
%security issue. Using the cost functions of corruption and penetration, based
%upon real world empirical data, they have shown that 10 and 30 days after the
%patch's release date are the optimal times to apply the patch to minimize the
%risk of defective patch. Such delay still opens a lot of opportunities for
%potential attackers.

%We believe that our approach can decrease this time virtually to zero
%eliminating the possibility of penetration while retaining the reliability of
%the original version.

%\begin{structure*}
%\item MonDe: Safe Updating through Monitored Deployment of New Component Versions
%\item Towards A Self-Managing Software Patching Process Using Balck-Box Persistent Manifests
%\item Predicting Problems Caused by Component Upgrades
%\item The Cracker Patch Choice: An Analysis of Post Hoc Security Techniques
%\item Yesterday, my program worked. Today, it does not. Why?
%\end{structure*}

%\subsection{Application Sandboxing and Software Fault Isolation}

%Multi-version execution requires some form of application sandboxing and
%software fault isolation. The goal is to isolate the running application from
%underlying system and especially, to prevent software failures from affecting
%the rest of the system including other applications.

%The use of software-based fault isolation for executing untrusted code has
%been described already in~\cite{wahbe1993} for the RISC machines with simple
%instruction set. On the other hand, the first effective implementation of
%software fault isolation for the CISC architecture has been shown much later
%in~\cite{mccamant2006}.

%% Control flow integrity?

%Douceur et. al shown in~\cite{douceur08} how to use sandboxing to enable
%leveraging of existing libraries and programs on the web. To achieve that,
%they used application-level virtualization; their implementation use system
%call mediation together with their own platform abstraction layer to run each
%library or program in so-called \emph{picoprocesses} which can be seen as
%stripped down virtual machine. The biggest downside of this approach is the
%need for code modifications which significantly reduces usability of this
%approach.

%Similar idea has been implemented by Yee et al. in~\cite{nacl} as an
%extension to Google Chrome web browser providing sandbox for untrusted native
%x86 code.  This can be used to develop web applications with performance of
%native applications. Their implementation consists of two parts --- inner
%sandbox which uses lightweight static analysis do detect security defects and
%outer sandbox which mediates system calls. The implementation has been further
%improved in~\cite{sehr2010} with support for ARM and x86-64 architectures and
%recently has been extended in~\cite{ansel2011} by adding support for
%Just-In-Time (JIT) compilation sandboxing. Again, the disadvantage here is the
%need for modifications of existing code and the use of modified GNU compiler
%toolchain to generate compliant binaries.

%\subsection{Virtualization and Fault-tolerant Computing}

%\begin{structure*}
  %\item Multiscale not Multicore: Efficient Heterogeneous Cloud Computing
  %\item Opportunistic Computing: A New Paradigm for Scalable Realism on Many-Cores
  %\item The Utility Coprocessor: Massively Parallel Computation from the Coffee Shop
%\end{structure*}

%Hardware-level virtualization has already been widely adopted in the industry
%for a variety of purposes.  Companies such as VMware or Microsoft provide a
%wide range of virtualization products, while several high-quality open-source
%solutions such as Xen~\cite{xen} also exist.
  
%Operating system-level virtualization is a method which allows to virtualize
%the operating system kernel into multiple isolated partitions (\ie user-space
%instances).  This type of virtualization is provided for different operating
%systems by products such as FreeBSD Jails~\cite{jails}, Linux
%VServer~\cite{vserver} or Solaris Containers~\cite{containers}. Even though
%these solutions are not as widely deployed as those for hardware-level
%virtualization, they are readily available, have lower overhead, and can be
%employed in many real-world scenarios.
  
%We aim to use \emph{application-level virtualization}, which is a lightweight
%variant of operating system-level virtualization, in which applications run in
%independent execution environments.  Even though this area has not been very
%intensively studied, many of its concepts, such as sandboxing, are becoming
%more and more common, especially in the cloud environment.   Some of these
%features are provided by the products such as VMware ThinApp~\cite{thinapp} or
%Microsoft App-V~\cite{appv}.

%Related research utilizing the concepts of application-level virtualization
%has been described in~\cite{yu2006} proposing lightweight virtual machine
%monitor for virtualization of Windows applications. The authors used namespace
%virtualization, implemented at system call level, to achieve isolation of
%resources between individual applications as well host operating system. This
%work has been followed up in~\cite{yu2008} showing three complete application
%benefiting from the approach presented in the original paper.

%More heavyweight, but also more powerful approach was taken by Dike et al.
%in~\cite{dike2001}. Using system call virtualization (via \texttt{ptrace}
%interface) and device abstraction, they managed to port Linux kernel to
%userspace. The resulting implementation runs a Linux virtual machine in a set
%of processes on a Linux host allowing various applications ranging from kernel
%development to application sandboxing and virtualization. Now, this
%implementation has become an optional module of Linux kernel.

%Virtualization approach is also often used in environments with strong
%reliability requirements, especially in the domain of cluster computing. We
%believe that our approach could be used also in these environments. Shown by
%Schroeder et al.  in~\cite{schroeder2007}, around 20\% of all failures at Los
%Alamos National Laboratory are caused by software failures making them a
%second largest contributor after hardware failures. While today's high
%performance computing systems relies mostly on checkpoint-restart schemes to
%achieve fault tolerance, such as the one proposed in~\cite{srinivasan2004}
%further used in~\cite{qin2005}. This approach is often not sufficient, mainly
%due to large decrease in the effective application utilization. Our approach,
%based on multi-version execution inspired by application-level virtualization
%idea, might present a viable alternative to these approaches.

%\begin{structure}
%\item Control-flow integrity
  %\begin{structure*}
  %\item Control-Flow Integrity
  %\item XFI: Software Guards for System Address Spaces
  %\end{structure*}
%\item Implementation related
  %\begin{structure*}
  %\item Rapid File System Development Using ptrace
  %\item Detours: Binary Interception of Win32 Functions
  %\end{structure*}
%\item Checkpointing and execution rollback
  %\begin{structure*}
  %\item Flashback: A Lightweight Extension for Rollback and Deterministic Replay for Software Debugging
  %\item Rx: Treating Bugs As Allergies
  %\item Making Applications Mobile Under Linux
  %\end{structure*}
%\end{structure}
