To evaluate our approach, we show that \mx can survive crash bugs in
several real applications: \gnu \coreutils
(\S\ref{sec:coreutils}), \redis (\S\ref{sec:redis}) and \lighttpd
(\S\ref{sec:lighttpd}).  We then examine the question of how far apart
can be the versions run by \mx (\S\ref{sec:bounds}), and discuss \mx's
performance overhead (\S\ref{sec:performance}).

\subsection{\coreutils}
\label{sec:coreutils}
\input{safe-updates/coreutils}

%\pagebreak
\subsection{\redis}
\label{sec:redis}
\input{safe-updates/redis}

%% \subsection{Md5sum}
%% \label{sec:mdsum}
%% %\input{mdsum}


\subsection{\lighttpd}
\label{sec:lighttpd}
\input{safe-updates/lighttpd}

\subsection{Ability to run distant versions}
\label{sec:bounds}

\begin{table}
\centering
\begin{tabular}{lcc}
\toprule
\textsc{Application Bug} & \textsc{Max distance} & \textsc{Time span} \\
\midrule
\lighttpd \#2169   & \maxDistLighttpdOne & \timeSpanLighttpdOne \\
\lighttpd \#2140   & \maxDistLighttpdTwo & \timeSpanLighttpdTwo \\
\redis \#344       & \maxDistRedis & \timeSpanRedis \\
%md5sum          & \maxDistMdsum & \timeSpanMdsum \\ \hline
\bottomrule
\end{tabular}
\caption{The maximum distance in number of revisions, and the time span
  between the revisions that can be run by \mx for each bug.}
\label{tbl:bug-bounds}
\end{table}

In the previous sections, we have shown how \mx can help software
survive crash bugs, by running two \textit{consecutive} versions of an
application, one which suffers from the bug, and one which does not.
%
One important question is how far apart can be the versions run
by \mx.  To answer this question, we determined for each of the bugs
discussed above the most distant revisions that can be run together to
survive the bug.  

For the \coreutils benchmarks, we are able to run versions which are
hundreds of revisions apart: \maxDistMdsum~revisions (corresponding to
\timeSpanMdsum of development time) for the \mdsum/\shasum bug; 
\maxDistMkdir~revisions (\timeSpanMkdir of development time) for the 
\mkdir/\mkfifo/\mknod bug; and \maxDistCut~revisions (\timeSpanCut 
of development time) for the \cut bug.

The most distant versions for the first \lighttpd bug are
approximately two months apart and have \maxDistLighttpdOne~revisions
in-between, while the most distant versions for the second
\lighttpd bug are also approximately two months apart but have only
\maxDistLighttpdTwo~revisions in-between.  Finally, the most distant
versions for the \redis bug are \maxDistRedis~revisions
and \timeSpanRedis apart.  

Of course, it is difficult to draw any general conclusions from only
this small number of data points.  Instead, we focus on understanding
the reasons why \mx couldn't run farther apart versions for the bugs
in \lighttpd and \redis (we ignore \coreutils, for which we can run
very distant versions).
%% For the \coreutils bugs, the lower bound is the earliest
%% version that we could build and run on our machine (v6.10).  The
%% upper-bound for 
%
For \lighttpd issue \#2169, the lower bound is defined by a revision
in which a pair of \textstt{geteuid()} and \textstt{getegid()} calls
are replaced with a single call to \textstt{issetugid()} to
allow \lighttpd to start for a non-root user with GID~0.  \mx 
%cannot run this revision together with the one before it, because it 
currently does not support changes to the order of system calls, but we believe
this limitation could be overcome by using peephole-style
optimisations~\cite{dragon-book}, which would allow \mx to recognise
that the pair \textstt{geteuid()} and \textstt{getegid()} could be
matched with the call to \textstt{issetugid()}.  The upper bound
for \lighttpd issue \#2169 adds a \textstt{read} call to
\textstt{/dev/[u]random}, in order to provide a better entropy
source for generating HTTP cookies.  This additional \textstt{read}
call changed the sequence of system calls, which \mx cannot
handle. \looseness=-1

For \lighttpd issue \#2140, both the lower and the upper bounds are
caused by a change in a sequence of \textstt{read()} system calls.  We
believe this could be optimised by allowing \mx to recognise when two
sequences of read system calls are used to perform the same overall
read.

%% Lower bound: the fix consists of request parser changes which resulted
%% in different sequence of \textstt{read()} system calls. The different
%% sequence of \textstt{read()} calls also marked the upper bound in this
%% case, defined by revision \lighttpdTwoUB. In this revision, the way in
%% which input connection buffer is being filled has changed, fixing
%% issue \#2147 and CVE-2010-0295 vulnerability.

For the \redis bug, the lower bound is given by the revision in which the
\textstt{HMGET} command was first implemented.  Since there was no support for
\textstt{HMGET} before that version, \mx has no way to survive the crash caused
by invoking \textstt{HMGET} with a wrong type (see \S\ref{sec:redis}).  The
upper bound is defined by a revision which changes the way error responses are
being constructed and reported, which results in a very different sequence of
system calls.

%% , including the call on line
%% \ref{line:report-error2} in Listing~\ref{lst:refactored}, resulting in
%% different sequence of system calls.

%% \todo{explain that all of these changes are minor and some of them could be
%% very well handled by using window-based/peep-hole approach}

\subsection{Performance Overhead}
\label{sec:performance}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{safe-updates/graphs/spec2006}
\caption{Normalised execution times for the \spec benchmark suite running under
\mx.}
\label{fig:spec}
\end{figure*}

We ran our experiments on a four-core server with 3.50~GHz Intel
Xeon E3-1280 and 16~GB of RAM running 64-bit Linux v3.1.9.

\textbf{\spec.}
To measure the performance overhead of our prototype, we first used
the standard \spec\footnote{\url{http://www.spec.org/cpu2006/}}
benchmark suite.  Figure~\ref{fig:spec} shows the performance of \mx
running two instances of the same application in parallel, compared to
a native system. The execution time overhead of \mx varies
from \minOverSPEC to \maxOverSPEC compared to executing just a single
version, with the geometric mean across all \numSPECbench benchmarks at
\avgOverSPEC. This result is comparable with previous work using multi-variant
execution that used SPEC CPU to measure performance~\cite{orchestra09} (even
though this work used SPEC~CPU2000 which has already been retired).

%% The overhead varies from \minRedisOver to \maxRedisOver depending
%% on the operation being performed. This is the worst case overhead
%% we have seen among all tested application and comes mainly from the
%% fact that \redis is an in-memory database optimised for maximum
%% bare-hardware performance and is very sensitive to any additional
%% software layers.  Even a state-of-the-art hypervisor can incur an
%% $n$-fold slowdown, so the relatively high measure overhead is
%% therefore unsurprising.

\textbf{\coreutils.} The six \coreutils applications discussed in 
\sref{sec:coreutils} are mostly used in an interactive fashion via the
command-line interface (CLI). For such applications, a high performance
overhead is acceptable as long as it is not perceptible to the user;
prior studies have shown that response times of less than 100ms
typically feel instantaneous~\cite{card:human_proc}. In many common use
cases (\eg creating a directory, or using \cut on a small text file),
the overhead of \mx was imperceptible---\eg creating a directory takes
around \avgMkdirNative natively and \avgMkdirMx with \mx. For the three
utilities that process files, we calculated the maximum file size for
which the response time with \mx stays under the 100ms threshold.  For
\cut, the maximum file size is \cutCutoffSize (with an overhead of
\cutCutoffOver), for \mdsum \mdsumCutoffSize (\mdsumCutoffOver
overhead), and for \shasum \shasumCutoffSize (\shasumCutoffOver
overhead).



\textbf{\redis and \lighttpd.} To measure the performance overhead for \redis, 
we used
the \redisbenchmark\footnote{\url{http://redis.io/topics/benchmarks}}
utility, which is part of the standard \redis distribution and
simulates \textstt{GET}/\textstt{SET} operations done by $N$ clients
concurrently, with default workload.  For \lighttpd, we used the
\httpload\footnote{\url{http://www.acme.com/software/http_load/}}
multiprocessing test client that is also used by the \lighttpd
developers.  Both of these standard benchmarks measure the end-to-end
time as perceived by users.  As a result, we performed two sets of
experiments: (1) with the client and server located on the same
machine, which represents the worst case performance-wise for \mx; and
(2) with the client and server located on different continents (one in
England and the other in California), which represents the best case.

The overhead for \redis varies, depending on the operation being
performed, from \minRedisRemote to \maxRedisRemote in the remote
scenario, and from \minRedisOver to \maxRedisOver in the local
scenario.  The overhead for \lighttpd varies from \minLighttpdRemote
to \maxLighttpdRemote in the remote scenario, and
from \minLighttpdOver to \maxLighttpdOver in the local scenario.
Despite the relatively large overhead in the local experiments, the
remote overhead is negligible because times are dominated by the
network latency (which in our case is over $150$ms).

As a result, we believe \mx is most suitable for scenarios for which
its execution overhead does not degrade the performance of the
end-to-end task, such as the remote \redis and \lighttpd scenarios
discussed above, or interactive tasks such as those performed using
command-line utilities, where users would not notice the overhead as
long as the response time stays within a certain range.

%% \mx's performance overhead is strongly correlated with the frequency of
%% system calls that have to be intercepted.  Therefore, we could also
%% imagine \mx being automatically turned on and off during execution,
%% depending on the frequency of system calls experienced by the
%% application.

Finally, we would like to emphasise that our current prototype has not
been optimised for performance, and we believe its overhead can still
be significantly reduced.  
%% There are multiple strategies we plan to explore in future
%% work. First,
For example, we could synchronise versions at a coarser granularity,
by using an epoch-based approach~\cite{compl-schedules11}, or we could
improve our checkpointing mechanism by implementing it as a loadable
kernel module that only stores the part of the state needed for
recovery~\cite{flashback}.

%% and only checkpoint at epoch boundaries.  To make this approach viable, we also
%% need to record system calls in each epoch, so that they can be
%% replayed during recovery. Second, 

%% instead of using \textstt{clone}
%% directly, we could implement the checkpointing functionality as a
%% loadable kernel module and only store the part of the state needed for
%% recovery as in~\cite{flashback}. Finally, we could explore the
%% possibility of not intercepting system calls in certain parts of the
%% code that were previously shown to be safe and do not need to be
%% replicated across multiple versions (\eg similarly
%% to~\cite{onlinevalidation}).

%The measured overhead is higher than
%for the SPEC~CPU2006 benchmarks (with a slowdown of up to \maxRedisOver
%for some operations in \redis) and we are currently investigating the
%reasons for this slowdown.

%First, we could to synchronise versions at a coarser
%granularity, by using a window/epoch approach~\cite{compl-schedules11},
%and by performing certain synchronisations at the level of shared
%library calls.  Second, we could explore the possibility of not
%intercepting system calls in certain parts of the code that were
%previously shown to be safe and do not need to be replicated across
%multiple versions.  Finally, we could decrease the checkpointing
%overhead, by performing them at a lower frequency, and record the
%external behaviour since the last checkpoint, so that it can be
%successfully replayed during recovery (\eg as in Rx~\cite{rx}).

We also examined how frequency of system calls affects the performance
overhead of application executed on top \mx. Figure~\ref{fig:syscall} shows
the average number of system calls during the execution of SPEC~CPU2006.
Rather surprising result is the fact that 452.libquantum, even though having
the largest run time overhead had the lowest average number of system calls.
On the hand, the performance overhead of 400.perlbench, despite having the
highest average number system, was bellow average.

\begin{figure}[ht]
\centering
\includegraphics[angle=270,width=\textwidth]{safe-updates/graphs/syscall}
\caption{Number of system calls made on average each second during the
execution of SPEC~CPU2006 benchmark suite, measured using \textstt{strace}
tool.}
\label{fig:syscall}
\end{figure}


% For example, as we discuss in related work, our
% monitor \mxm is very similar to the monitor used by
% Orchestra~\cite{orchestra09}, which by employing various optimisations
% manages to obtain an average overhead of only about 15\% when
% synchronising two program variants at the level of system calls.  In
% terms of checkpointing, the Rx system~\cite{rx} implements a similar
% approach based on the Linux copy-on-write mechanism, and which through
% various optimisations manages to achieve a performance penalty of less
% than 5\% when checkpointing every 200 milliseconds.
