\section{Evaluation}
\label{sec:reliability-evaluation}

To evaluate our approach, we show that \mx can survive crash bugs in several
applications. %(\S\ref{sec:surviving}). We then examine the question of how
%far apart can be the versions run by \mx (\S\ref{sec:bounds}), and discuss
%\mx's performance overhead (\S\ref{sec:performance}).
We have evaluated \mx using a set of bugs in three applications: \gnu
\coreutils, \redis and \lighttpd. We discuss each application in turn below.

\subsubsection{\gnu \coreutils}
\label{sec:coreutils}

\begin{table}[t]
\begin{center}
\caption{Utilities from \gnu \coreutils, the crash bugs used, and the 
versions in which these bugs were introduced and fixed.  We group
together utilities affected by the same or similar bugs.}
\begin{tabular}{lll}
\toprule
\textsc{Utility} & \textsc{Bug description} & \textsc{Bug span} \\
\midrule
\mdsum & \multirow{2}{*}{Buffer underflow} & \multirow{2}{*}{v5.1 -- v6.11} \\
\shasum & & \\
\midrule
\mkdir & \multirow{2}{*}{\textstt{NULL}-pointer dereference} & \multirow{2}{*}{v5.1 -- v6.11} \\
\mkfifo & & \\
\mknod & & \\
\midrule
\cut & Buffer overflow & v5.3 -- v8.11 \\
\bottomrule
\end{tabular}
\label{tbl:cu-bugs}
\end{center}
\end{table}

The \gnu \coreutils utility
suite\footnote{\url{http://www.gnu.org/software/coreutils/}} provides
the core user-level environment on most UNIX systems.  We have selected
a number of bugs reported on the \coreutils mailing list, all of which
trigger segmentation faults.  The bugs are described in
Table~\ref{tbl:cu-bugs}, together with the utilities affected by each
bug and the versions in which they were introduced and fixed.

The bug affecting both \mdsum and \shasum utilities, introduced in version 5.1
and later fixed in version 6.11, caused a crash due to a buffer underflow when
checking an invalid BSD-style input. Another bug we have considered affected
\mkdir, \mkfifo and \mknod utilities; this bug, which was reported in 6.10 and
fixed in 6.11 resulted in a crash when accessing an invalid context.  Finally,
the bug affecting the \cut utility, introduced in 5.3 and later fixed in 8.11,
resulted in a segmentation fault when using a large unbounded range. 

For all these bugs, we configured \mx to run the version that fixed the
bug together with the one just before. We could have also run the
version that introduced the bug with the one just before, but we could
not immediately tell where the bug was introduced, and we cannot build
versions earlier than 6.10 due to changes in GCC and GNU C library. \mx
successfully intercepted the crash and recovered the execution by using
the strategy described in Section~\ref{sec:rem}.

We discuss below the bug in the \cut utility (used to remove sections
from each line of a file), triggered by the following invocation:

\begin{lstlisting}[numbers=none,breaklines=true,xleftmargin=0pt,language=bash]
cut -c1234567890- --output-d=: foo
\end{lstlisting}

This bug is triggered by the conditional statement on line~\ref{line:cond}:

\begin{lstlisting}[firstnumber=525]
if (output_delimiter_specified /*@\label{line:cond}@*/
    && !complement
    && eol_range_start && !is_printable_field (rsi_candidate))
\end{lstlisting}

This code uses the lower bound of the size of the printable field
vector; however, when calculating the size of this vector, ranges going
to the end of line (\ie \lstinline`0-`) are not considered, eventually
resulting in an invalid memory access. The cause of this bug is an incorrectly
calculated size of a dynamically allocated buffer, which is later used by the
program, leading to a buffer overflow. When \mx intercepts this bug, it uses
the last checkpoint to recover the execution of the crashing version. This
checkpoint is taken after the \lstinline`brk` system call triggered by the
\lstinline`malloc` call that allocates the buffer in function
\lstinline`bindtextdomain` on line~\ref{line:bind}.

\begin{lstlisting}[firstnumber=767]
bindtextdomain (PACKAGE, LOCALEDIR); /*@\label{line:bind}@*/
\end{lstlisting}

The recovered process uses the code of the other version to correctly
calculate the size of the field vector and switches back to the original
code during the allocation of this buffer, as
%code during the allocation of this buffer on line~\ref{line:alloc} as
function \lstinline`xzalloc` triggers a \lstinline`mmap64` system call just
before executing the conditional statement on line~\ref{line:cond},
which originally triggered the bug.

\subsubsection{\redis}
\label{sec:redis}

Below, we describe how \mx can survive the \redis bug presented in
Section~\ref{multi-version:scenarios}; this bug, introduced during a code
refactoring, causes \redis to crash when the \lstinline`HMGET` command is used
with the wrong type. We are running in parallel the revision \textstt{a71f072f}
(\textit{the old version}, just before the bug was introduced) with revision
\textstt{7fb16bac} (\textit{the new version}, which contains the bug).  \mx
first invokes \sea to perform a static analysis of the two binaries and
construct the mappings described in \sref{sec:sea}.  Then, \mx invokes the \mxm
monitor, which executes both versions as child processes and intercepts their
system calls.

When the new version crashes after issuing the problematic
\textstt{HMGET} command, \mxm intercepts the \textstt{SIGSEGV} signal
which is sent to the application by the operating system.  At
this point, \rem starts the recovery procedure.  First, \rem sends a
\textstt{SIGKILL} signal to the new version to terminate it.  It then
restores the last checkpoint of the new version, which was taken at the
point of the last invoked system call, which in this case is an
\textstt{epoll\_ctl} system call.  Then, \rem uses the information
provided by \sea to rewrite the stack of the new version, as detailed
in \sref{sec:rem}.  In particular, \rem replaces the return
addresses of all functions in the new version with the corresponding
addresses from the old version. The stack rewriting itself however is not
enough. The newer version can still use function pointers, which are
part of the replicated state, to invoke the original code. To prevent
this situation, \rem inserts breakpoints at the beginning of all the
functions in the code of the new version (to intercept indirect calls
via function pointers), and then finally restores the original processor
registers of the checkpointed process and restarts the execution of the
(modified) new version.

Since the checkpoint was performed right after the execution of the system
call \textstt{epoll\_ctl}, the first thing that the code does is to
return from the \textstt{libc} wrapper that performed this system
call.  This in turn will return to the corresponding code in the old
version that invoked the wrapper, since all return addresses on the
stack have been rewritten.  From then on, the code of the old version
is executed (but in the state of the new version), until the first
system call is intercepted.  In our example, the old and the new
versions perform the same system call (and with the same arguments),
so \rem concludes that the two processes have re-converged, and thus
restores back the code of the new version by performing the steps
above in reverse, plus the additional step of synchronising their
global state (see \S\ref{sec:rem}).  Finally, the control is handed
back to the \mxm monitor, which continues to monitor the execution of
the two versions.

\subsubsection{\lighttpd}
\label{sec:lighttpd}

To evaluate \mx on \lighttpd, we have used two different crash bugs, \#1601 and
\#2140.  The first bug is the one described in detail in Section
\ref{multi-version:scenarios}, related to the ETag and compression
functionalities.  As previously discussed, the crash is triggered by a very
small change, which decrements the upper bound of a \textstt{for} loop by one.
\mx successfully protects the application against this crash, and allows the
new version to survive it by using the code of the old version.

The other crash bug we reproduced affects the URL rewrite
functionality.\footnote{\url{http://redmine.lighttpd.net/projects/lighttpd/issues/2140}}
This is also caused by an incorrect bound in a \lstinline`for` loop.
More precisely, the loop: 

\begin{lstlisting}[numbers=none,breaklines=true,xleftmargin=0pt]
for (k=0; k < pattern_len; k++)
\end{lstlisting}

\noindent should have been:

\begin{lstlisting}[numbers=none,breaklines=true,xleftmargin=0pt]
for (k=0; k@+1@ < pattern_len; k++)
\end{lstlisting}

The bug seems to have been present since the very first version
added to the repository.  It was reported in December 2009, and
fixed one month later.  As a result, we are running \mx using the last
version containing the bug together with the one that fixed it.  While
this bug does not fit within the pattern targeted by \mx (where a
newer revision introduces the bug), from a technical perspective it is
equally challenging.  \mx is able to successfully run the two versions
in parallel, and help the old version survive the crash bug.

%The bug \#1601 affects the HTTP redirection functionality, in particular
%the \texttt{\%n} substitution with condition substring. This functionality has
%been introduced in revision \texttt{510}. However, there is an incorrect
%comparison in one of the conditions which causes segmentation fault when
%appending matched parts to buffer if there was no matching regular expression.
%The affected code can be seen in Listing~\ref{lst:510}.

%\begin{lstlisting}[label=lst:510, caption={Original correct version of the function}]
%cond_cache_t *cache = &con->cond_cache[dc->context_ndx];
%if (n > cache->patterncount) {
  %return 0;
%}
%\end{lstlisting}

%The fix to this bug consists of a single changed line as can be seen in
%Listing~\ref{lst:2138} and has been incorporated in revision \texttt{2138}, yet
%this bug remained undetected for nearly three years (August 8, 2005 --- March
%26, 2008) rendering \lighttpd webserver vulnerable to attack.

%\begin{lstlisting}[label=lst:2138, caption={Refactored failing version of the function}]
%cond_cache_t *cache = &con->cond_cache[dc->context_ndx];
%if (n >= cache->patterncount) {
  %return 0;
%}
%\end{lstlisting}

Both \lighttpd bugs \#1601 and \#2140 are very simple---their fix consist of a
single character---yet still they made the \lighttpd server vulnerable to a
potential attack. \mx can not only handle the crash, but also successfully
recover the failing version in both cases.

\subsection{Ability to run distant versions}
\label{sec:bounds}

\begin{table}
\begin{center}
\caption{The maximum distance in number of revisions, and the time span
  between the revisions that can be run by \mx for each bug.}
\begin{tabular}{lcc}
\toprule
\textsc{Application Bug} & \textsc{Max distance} & \textsc{Time span} \\
\midrule
\lighttpd \#2169   & \maxDistLighttpdOne & \timeSpanLighttpdOne \\
\lighttpd \#2140   & \maxDistLighttpdTwo & \timeSpanLighttpdTwo \\
\redis \#344       & \maxDistRedis & \timeSpanRedis \\
%md5sum          & \maxDistMdsum & \timeSpanMdsum \\ \hline
\bottomrule
\end{tabular}
\label{tbl:bug-bounds}
\end{center}
\end{table}

In the previous sections, we have shown how \mx can help software
survive crash bugs, by running two \textit{consecutive} versions of an
application, one which suffers from the bug, and one which does not.
%
One important question is how far apart can be the versions run
by \mx.  To answer this question, we determined for each of the bugs
discussed above the most distant revisions that can be run together to
survive the bug.  

For the \coreutils benchmarks, we are able to run versions which are
hundreds of revisions apart: \maxDistMdsum~revisions (corresponding to
\timeSpanMdsum of development time) for the \mdsum/\shasum bug; 
\maxDistMkdir~revisions (\timeSpanMkdir of development time) for the 
\mkdir/\mkfifo/\mknod bug; and \maxDistCut~revisions (\timeSpanCut 
of development time) for the \cut bug.

The most distant versions for the first \lighttpd bug are
approximately two months apart and have \maxDistLighttpdOne~revisions
in-between, while the most distant versions for the second
\lighttpd bug are also approximately two months apart but have only
\maxDistLighttpdTwo~revisions in-between.  Finally, the most distant
versions for the \redis bug are \maxDistRedis~revisions
and \timeSpanRedis apart.  

Of course, it is difficult to draw any general conclusions from only
this small number of data points.  Instead, we focus on understanding
the reasons why \mx could not run farther apart versions for the bugs
in \lighttpd and \redis (we ignore \coreutils, for which we can run
very distant versions).
%% For the \coreutils bugs, the lower bound is the earliest
%% version that we could build and run on our machine (v6.10).  The
%% upper-bound for 
%
For \lighttpd issue \#2169, the lower bound is defined by a revision
in which a pair of \lstinline`geteuid` and \lstinline`getegid` calls
are replaced with a single call to \lstinline`issetugid` to
allow \lighttpd to start for a non-root user with GID~\lstinline`0`. \mx 
currently does not support changes to the order of system calls, but
this limitation could be overcome by using the rewrite rules for system call
sequences supported  by \varan (\S\ref{sec:patternmatching}). This would
allow \mx to recognise that the pair \textstt{geteuid()} and
\textstt{getegid()} could be matched with the call to
\textstt{issetugid()} (\S\ref{sec:applications}).

The upper bound for \lighttpd issue \#2169 adds a \textstt{read} call to
\textstt{/dev/[u]random}, in order to provide a better entropy source for
generating HTTP cookies.  This additional \textstt{read} call changed the
sequence of system calls, which \mx cannot handle, but which again could be
handled by \varan's rewriting rules.

For \lighttpd issue \#2140, both the lower and the upper bounds are
caused by a change in a sequence of \textstt{read()} system calls.  We
believe this could be optimised by allowing \mx to recognise when two
sequences of read system calls are used to perform the same overall
read.

%% Lower bound: the fix consists of request parser changes which resulted
%% in different sequence of \textstt{read()} system calls. The different
%% sequence of \textstt{read()} calls also marked the upper bound in this
%% case, defined by revision \lighttpdTwoUB. In this revision, the way in
%% which input connection buffer is being filled has changed, fixing
%% issue \#2147 and CVE-2010-0295 vulnerability.

For the \redis bug, the lower bound is given by the revision in which the
\textstt{HMGET} command was first implemented.  Since there was no support for
\textstt{HMGET} before that version, \mx has no way to survive the crash caused
by invoking \textstt{HMGET} with a wrong type (see \S\ref{sec:redis}).  The
upper bound is defined by a revision which changes the way error responses are
being constructed and reported, which results in a very different sequence of
system calls.

%% , including the call on line
%% \ref{line:report-error2} in Listing~\ref{lst:refactored}, resulting in
%% different sequence of system calls.

%% \todo{explain that all of these changes are minor and some of them could be
%% very well handled by using window-based/peep-hole approach}

\subsection{Performance overhead}
\label{sec:performance}

\begin{figure}[!t]
\centering
\includegraphics[width=\textwidth]{safe-updates/graphs/spec2006}
\caption{Normalised execution times for the \speczerosix benchmark suite
running under \mx.}
\label{fig:spec}
\end{figure}

We ran our experiments on a four-core server with 3.50~GHz Intel
Xeon E3-1280 and 16~GB of RAM running 64-bit Linux v3.1.9.

\begin{enumerate}
\item[\speczerosix] To measure the performance overhead of our prototype, we
  first used the standard
  \speczerosix\footnote{\url{http://www.spec.org/cpu2006/}} benchmark suite.
  Figure~\ref{fig:spec} shows the performance of \mx running two instances of
  the same application in parallel, compared to a native system. The execution
  time overhead of \mx varies from \minOverSPEC to \maxOverSPEC compared to
  executing just a single version, with the geometric mean across all
  \numSPECbench benchmarks at \avgOverSPEC. This result is comparable with
  previous work using multi-variant execution that used \speccpu to measure
  performance~\cite{orchestra09} (even though that work used \speczerozero).

%% The overhead varies from \minRedisOver to \maxRedisOver depending
%% on the operation being performed. This is the worst case overhead
%% we have seen among all tested application and comes mainly from the
%% fact that \redis is an in-memory database optimised for maximum
%% bare-hardware performance and is very sensitive to any additional
%% software layers.  Even a state-of-the-art hypervisor can incur an
%% $n$-fold slowdown, so the relatively high measure overhead is
%% therefore unsurprising.

\item[\gnu~\coreutils] The six \coreutils applications discussed in
  \sref{sec:coreutils} are mostly used in an interactive fashion via the
  command-line interface (CLI). In this context, a high performance
  overhead is acceptable as long as it is not perceptible to the user; prior
  studies have shown that response times of less than 100ms typically feel
  instantaneous~\cite{card:human_proc}. In many common use cases (\eg creating
  a directory, or using \cut on a small text file), the overhead of \mx was
  imperceptible---\eg creating a directory takes around \avgMkdirNative
  natively and \avgMkdirMx with \mx. For the three utilities that process
  files, we calculated the maximum file size for which the response time with
  \mx stays under the 100ms threshold.  For \cut, the maximum file size is
  \cutCutoffSize (with an overhead of \cutCutoffOver), for \mdsum
  \mdsumCutoffSize (\mdsumCutoffOver overhead), and for \shasum
  \shasumCutoffSize (\shasumCutoffOver overhead).

\item[\lighttpd] We used the
  \httpload\footnote{\url{http://www.acme.com/software/http_load/}}
  multiprocessing test client that is also used by the \lighttpd developers.
  This benchmark measures the end-to-end time as perceived by users. As a
  result, we performed two sets of experiments: (1) with the client and server
  located on the same machine, which represents the worst case performance-wise
  for \mx; and (2) with the client and server located on different continents
  (one in England and the other in California), which represents the best case.
  The overhead for \lighttpd varies from \minLighttpdRemote to
  \maxLighttpdRemote in the remote scenario, and from \minLighttpdOver to
  \maxLighttpdOver in the local scenario.  Despite the relatively large
  overhead in the local experiments, the remote overhead is negligible because
  times are dominated by the network latency (which in our case is over
  \SI{150}{\milli\second}).

\item[\redis] To measure the performance overhead for \redis, we used the
  \redisbenchmark\footnote{\url{http://redis.io/topics/benchmarks}} utility,
  which is part of the standard \redis distribution and simulates
  \textstt{GET}/\textstt{SET} operations done by $N$ clients concurrently, with
  default workload. As in the case of \lighttpd, we performed two sets of
  experiments, one for the local scenario and one for the remote scenario. The
  overhead for \redis varies, depending on the operation being performed, from
  \minRedisRemote to \maxRedisRemote in the remote scenario, and from
  \minRedisOver to \maxRedisOver in the local scenario. 

\end{enumerate}

Given the results presented hereinbefore, we believe \mx is most suitable for
scenarios for which its execution overhead does not degrade the performance of
the end-to-end task, such as the remote \redis and \lighttpd scenarios
discussed above, or interactive tasks such as those performed using
command-line utilities, where users would not notice the overhead as long as
the response time stays within a certain range. On the other hand, it is
clearly not suitable for applications requiring high-throughput, which is what
motivated us design the high-performance mechanism of \varan.

%% \mx's performance overhead is strongly correlated with the frequency of
%% system calls that have to be intercepted.  Therefore, we could also
%% imagine \mx being automatically turned on and off during execution,
%% depending on the frequency of system calls experienced by the
%% application.

%% and only checkpoint at epoch boundaries.  To make this approach viable, we also
%% need to record system calls in each epoch, so that they can be
%% replayed during recovery. Second, 

%% instead of using \textstt{clone}
%% directly, we could implement the checkpointing functionality as a
%% loadable kernel module and only store the part of the state needed for
%% recovery as in~\cite{flashback}. Finally, we could explore the
%% possibility of not intercepting system calls in certain parts of the
%% code that were previously shown to be safe and do not need to be
%% replicated across multiple versions (\eg similarly
%% to~\cite{onlinevalidation}).

%The measured overhead is higher than
%for the SPEC~CPU2006 benchmarks (with a slowdown of up to \maxRedisOver
%for some operations in \redis) and we are currently investigating the
%reasons for this slowdown.

%First, we could to synchronise versions at a coarser
%granularity, by using a window/epoch approach~\cite{compl-schedules11},
%and by performing certain synchronisations at the level of shared
%library calls.  Second, we could explore the possibility of not
%intercepting system calls in certain parts of the code that were
%previously shown to be safe and do not need to be replicated across
%multiple versions.  Finally, we could decrease the checkpointing
%overhead, by performing them at a lower frequency, and record the
%external behaviour since the last checkpoint, so that it can be
%successfully replayed during recovery (\eg as in Rx~\cite{rx}).

%We also examined how the frequency of system calls affects the
%performance overhead of application executed on top \mx.
%Figure~\ref{fig:syscall} shows the average number of system calls during
%the execution of SPEC~CPU2006.  Rather surprising result is the fact
%that \textsf{452.libquantum}, even though having the largest run time
%overhead had the lowest average number of system calls.  On the hand,
%the performance overhead of \textsf{400.perlbench}, despite having the
%highest average number system, was bellow average. \todo{What is the
%conclusion here?}

% For example, as we discuss in related work, our
% monitor \mxm is very similar to the monitor used by
% Orchestra~\cite{orchestra09}, which by employing various optimisations
% manages to obtain an average overhead of only about 15\% when
% synchronising two program variants at the level of system calls.  In
% terms of checkpointing, the Rx system~\cite{rx} implements a similar
% approach based on the Linux copy-on-write mechanism, and which through
% various optimisations manages to achieve a performance penalty of less
% than 5\% when checkpointing every 200 milliseconds.
