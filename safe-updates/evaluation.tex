\section{Reliability evaluation}
\label{sec:reliability-evaluation}

To evaluate our approach, we show that \mx can survive crash bugs in several
real applications (\S\ref{sec:surviving}). We then examine the question of how
far apart can be the versions run by \mx (\S\ref{sec:bounds}), and discuss
\mx's performance overhead (\S\ref{sec:performance}).

\subsection{Fault recovery}
\label{sec:surviving}

We have evaluated \mx using a set of bugs in three applications: \gnu
\coreutils, \redis and \lighttpd. We discuss each application in turn below.

\subsubsection{\gnu \coreutils}
\label{sec:coreutils}

\begin{table}[t]
\begin{center}
\caption{Utilities from \gnu \coreutils, the crash bugs used, and the 
versions in which these bugs were introduced and fixed.  We group
together utilities affected by the same or similar bugs.}
\begin{tabular}{lll}
\toprule
\textsc{Utility} & \textsc{Bug description} & \textsc{Bug span} \\
\midrule
\mdsum & \multirow{2}{*}{Buffer underflow} & \multirow{2}{*}{v5.1 -- v6.11} \\
\shasum & & \\
\midrule
\mkdir & \multirow{2}{*}{\textstt{NULL}-pointer dereference} & \multirow{2}{*}{v5.1 -- v6.11} \\
\mkfifo & & \\
\mknod & & \\
\midrule
\cut & Buffer overflow & v5.3 -- v8.11 \\
\bottomrule
\end{tabular}
\label{tbl:cu-bugs}
\end{center}
\end{table}

As an initial evaluation of \mx's ability to survive crashes, we have used
applications from the \gnu \coreutils utility
suite,\footnote{\url{http://www.gnu.org/software/coreutils/}} which provides
the core user-level environment on most UNIX systems.  We have selected a
number of bugs reported on the \coreutils mailing list, all of which trigger
segmentation faults.  The bugs are described in Table~\ref{tbl:cu-bugs},
together with the utilities affected by each bug and the versions in which they
were introduced and fixed.

The bug affecting both \mdsum and \shasum utilities introduced in 5.1 and later
fixed in 6.11 caused a crash due to buffer underflow when checking an invalid
BSD-style input. Another bug we have considered affected \mkdir, \mkfifo and
\mknod utilities; this bug, which was reported in 6.10 and fixed in 6.11
resulted in crash when diagnosing invalid context.  Finally, the bug affecting
\cut utility, introduced in 5.3 and later fixed in 8.11, resulted in segfault
when using large unbounded range. 

For all these bugs, we configured \mx to run the version that fixed the bug
together with the one just before.  (we could have also run the version that
introduced the bug with the one just before, but we could not immediately tell
where the bug was introduced, and we cannot build versions earlier than 6.10
due to changes in GCC and GNU C library).  \mx successfully intercepted the
crash and recovered the execution by using the strategy described in
\sref{sec:rem}.

We discuss below the bug in the \cut utility (used to remove sections from each
line of file), triggered by the following invocation:

\begin{lstlisting}[numbers=none,breaklines=true,xleftmargin=0pt,language=bash]
cut -c1234567890- --output-d=: foo
\end{lstlisting}

This bug is triggered by the conditional statement on line~\ref{line:cond}:

\begin{lstlisting}[firstnumber=525]
if (output_delimiter_specified /*@\label{line:cond}@*/
    && !complement
    && eol_range_start && !is_printable_field (rsi_candidate))
\end{lstlisting}

This code uses the lower bound of the size of the printable field
vector; however, when calculating the size of this vector, ranges going
to the end of line (\ie \lstinline`0-`) are not considered eventually
resulting in invalid memory access. 
% The bug is caused by a buffer overflow whose root cause is the
% incorrect calculation of the size of a dynamically allocated buffer
% used internally by \cut.
When \mx intercepts this bug, it uses the
last checkpoint to recover the execution of the crashing version. This
checkpoint is taken after the \textstt{brk} system call triggered by
the \textstt{malloc} call that allocates the buffer. 
% in function \textstt{bindtextdomain} on line~\ref{line:bind}.
% \begin{lstlisting}[firstnumber=767]
% bindtextdomain (PACKAGE, LOCALEDIR); /*@\label{line:bind}@*/
% \end{lstlisting}
The recovered process uses the code of the other version to correctly
calculate the size of the field vector and switches back to the original
code during the allocation of this buffer as
%code during the allocation of this buffer on line~\ref{line:alloc} as
function \textstt{xzalloc} triggers \textstt{mmap64} system call, just
before executing the conditional statement on line~\ref{line:cond},
which originally triggered the bug.

\subsubsection{\redis}
\label{sec:redis}

Below, we describe how \mx can survive the \redis bug described in
Section~\ref{multi-version:scenarios} while running in parallel the \redis
revision \textstt{a71f072f} (\textit{the old version}, just before the bug was
introduced) with revision \textstt{7fb16bac} (\textit{the new version}, just
after the bug).  \mx first invokes \sea to perform a static analysis of the two
binaries and construct the mappings described in \sref{sec:sea}.  Then, \mx
invokes the \mxm monitor, which executes both versions as child processes and
intercepts their system calls.

When the new version crashes after issuing the problematic
\textstt{HMGET} command, \mxm intercepts the \textstt{SIGSEGV} signal
which is sent to the application by the operating system.  At
this point, \rem starts the recovery procedure.  First, \rem sends a
\textstt{SIGKILL} signal to the new version to terminate it.  It then
takes the last checkpoint of the new version, which was taken at the
point of the last invoked system call, which in this case is an
\textstt{epoll\_ctl} system call.  Then, \rem uses the information
provided by \sea to rewrite the stack of the new version, as detailed
in \sref{sec:rem}.  In particular, \rem replaces the return
addresses of all functions in the new version with the corresponding
addresses from the old version. The stack rewriting itself however is not
enough. The newer version can still use function pointers, which are part of
the replica state, to invoke the original code. To prevent this situation, \rem
inserts breakpoints at the beginning of all the functions in the code of the
new version (to intercept indirect calls via function pointers), and then
finally restores the original processor registers of the checkpointed process
and restarts the execution of the (modified) new version.

Since the checkpoint was performed right after the execution of the system
call \textstt{epoll\_ctl}, the first thing that the code does is to
return from the \textstt{libc} wrapper that performed this system
call.  This in turn will return to the corresponding code in the old
version that invoked the wrapper, since all return addresses on the
stack have been rewritten.  From then on, the code of the old version
is executed (but in the state of the new version), until the first
system call is intercepted.  In our example, the old and the new
versions perform the same system call (and with the same arguments),
so \rem concludes that the two processes have re-converged, and thus
restores back the code of the new version by performing the steps
above in reverse, plus the additional step of synchronising their
global state (see \S\ref{sec:rem}).  Finally, the control is handed
back to the \mxm monitor, which continues to monitor the execution of
the two versions.

\subsubsection{\lighttpd}
\label{sec:lighttpd}

To evaluate \mx on \lighttpd, we have used two different crash bugs.
The first bug is the one described in detail in
Section \ref{multi-version:scenarios}, related to the ETag and compression
functionalities.  As previously discussed, the crash is triggered by a
very small change, which decrements the upper bound of a \textstt{for}
loop by one.  \mx successfully protects the application against this
crash, and allows the new version to survive it by using the code of
the old version.

The other crash bug we reproduced affects the URL rewrite
functionality.\footnote{\url{http://redmine.lighttpd.net/projects/lighttpd/issues/2140}}
This is also caused by an incorrect bound in a \lstinline`for` loop.
More precisely, the loop: 

\begin{lstlisting}[numbers=none,breaklines=true,xleftmargin=0pt]
for (k=0; k < pattern_len; k++)
\end{lstlisting}

\noindent should have been:

\begin{lstlisting}[numbers=none,breaklines=true,xleftmargin=0pt]
for (k=0; k@+1@ < pattern_len; k++)
\end{lstlisting}

The bug seems to have been present since the very first version
added to the repository.  It was reported in December 2009, and
fixed one month later.  As a result, we are running \mx using the last
version containing the bug together with the one that fixed it.  While
this bug does not fit within the pattern targeted by \mx (where a
newer revision introduces the bug), from a technical perspective it is
equally challenging.  \mx is able to successfully run the two versions
in parallel, and help the old version survive the crash bug.

%The bug \#1601 affects the HTTP redirection functionality, in particular
%the \texttt{\%n} substitution with condition substring. This functionality has
%been introduced in revision \texttt{510}. However, there is an incorrect
%comparison in one of the conditions which causes segmentation fault when
%appending matched parts to buffer if there was no matching regular expression.
%The affected code can be seen in Listing~\ref{lst:510}.

%\begin{lstlisting}[label=lst:510, caption={Original correct version of the function}]
%cond_cache_t *cache = &con->cond_cache[dc->context_ndx];
%if (n > cache->patterncount) {
  %return 0;
%}
%\end{lstlisting}

%The fix to this bug consists of a single changed line as can be seen in
%Listing~\ref{lst:2138} and has been incorporated in revision \texttt{2138}, yet
%this bug remained undetected for nearly three years (August 8, 2005 --- March
%26, 2008) rendering \lighttpd webserver vulnerable to attack.

%\begin{lstlisting}[label=lst:2138, caption={Refactored failing version of the function}]
%cond_cache_t *cache = &con->cond_cache[dc->context_ndx];
%if (n >= cache->patterncount) {
  %return 0;
%}
%\end{lstlisting}

Both \lighttpd bugs \#1601 and \#2140 are very simple---their fix
consist of a single character---yet still they made \lighttpd server
vulnerable to a potential attack. \mx can not only handle the crash,
but also successfully recover the failing version in both cases.

\subsection{Ability to run distant versions}
\label{sec:bounds}

\begin{table}
\begin{center}
\caption{The maximum distance in number of revisions, and the time span
  between the revisions that can be run by \mx for each bug.}
\begin{tabular}{lcc}
\toprule
\textsc{Application Bug} & \textsc{Max distance} & \textsc{Time span} \\
\midrule
\lighttpd \#2169   & \maxDistLighttpdOne & \timeSpanLighttpdOne \\
\lighttpd \#2140   & \maxDistLighttpdTwo & \timeSpanLighttpdTwo \\
\redis \#344       & \maxDistRedis & \timeSpanRedis \\
%md5sum          & \maxDistMdsum & \timeSpanMdsum \\ \hline
\bottomrule
\end{tabular}
\label{tbl:bug-bounds}
\end{center}
\end{table}

In the previous sections, we have shown how \mx can help software
survive crash bugs, by running two \textit{consecutive} versions of an
application, one which suffers from the bug, and one which does not.
%
One important question is how far apart can be the versions run
by \mx.  To answer this question, we determined for each of the bugs
discussed above the most distant revisions that can be run together to
survive the bug.  

For the \coreutils benchmarks, we are able to run versions which are
hundreds of revisions apart: \maxDistMdsum~revisions (corresponding to
\timeSpanMdsum of development time) for the \mdsum/\shasum bug; 
\maxDistMkdir~revisions (\timeSpanMkdir of development time) for the 
\mkdir/\mkfifo/\mknod bug; and \maxDistCut~revisions (\timeSpanCut 
of development time) for the \cut bug.

The most distant versions for the first \lighttpd bug are
approximately two months apart and have \maxDistLighttpdOne~revisions
in-between, while the most distant versions for the second
\lighttpd bug are also approximately two months apart but have only
\maxDistLighttpdTwo~revisions in-between.  Finally, the most distant
versions for the \redis bug are \maxDistRedis~revisions
and \timeSpanRedis apart.  

Of course, it is difficult to draw any general conclusions from only
this small number of data points.  Instead, we focus on understanding
the reasons why \mx could not run farther apart versions for the bugs
in \lighttpd and \redis (we ignore \coreutils, for which we can run
very distant versions).
%% For the \coreutils bugs, the lower bound is the earliest
%% version that we could build and run on our machine (v6.10).  The
%% upper-bound for 
%
For \lighttpd issue \#2169, the lower bound is defined by a revision
in which a pair of \textstt{geteuid()} and \textstt{getegid()} calls
are replaced with a single call to \textstt{issetugid()} to
allow \lighttpd to start for a non-root user with GID~0.  \mx 
%cannot run this revision together with the one before it, because it 
currently does not support changes to the order of system calls, but we believe
this limitation could be overcome by using peephole-style
optimisations~\cite{dragon-book}, which would allow \mx to recognise
that the pair \textstt{geteuid()} and \textstt{getegid()} could be
matched with the call to \textstt{issetugid()}.  The upper bound
for \lighttpd issue \#2169 adds a \textstt{read} call to
\textstt{/dev/[u]random}, in order to provide a better entropy
source for generating HTTP cookies.  This additional \textstt{read}
call changed the sequence of system calls, which \mx cannot
handle. \looseness=-1

For \lighttpd issue \#2140, both the lower and the upper bounds are
caused by a change in a sequence of \textstt{read()} system calls.  We
believe this could be optimised by allowing \mx to recognise when two
sequences of read system calls are used to perform the same overall
read.

%% Lower bound: the fix consists of request parser changes which resulted
%% in different sequence of \textstt{read()} system calls. The different
%% sequence of \textstt{read()} calls also marked the upper bound in this
%% case, defined by revision \lighttpdTwoUB. In this revision, the way in
%% which input connection buffer is being filled has changed, fixing
%% issue \#2147 and CVE-2010-0295 vulnerability.

For the \redis bug, the lower bound is given by the revision in which the
\textstt{HMGET} command was first implemented.  Since there was no support for
\textstt{HMGET} before that version, \mx has no way to survive the crash caused
by invoking \textstt{HMGET} with a wrong type (see \S\ref{sec:redis}).  The
upper bound is defined by a revision which changes the way error responses are
being constructed and reported, which results in a very different sequence of
system calls.

%% , including the call on line
%% \ref{line:report-error2} in Listing~\ref{lst:refactored}, resulting in
%% different sequence of system calls.

%% \todo{explain that all of these changes are minor and some of them could be
%% very well handled by using window-based/peep-hole approach}

\subsection{Performance overhead}
\label{sec:performance}

\begin{figure}[!t]
\centering
\includegraphics[width=\textwidth]{safe-updates/graphs/spec2006}
\caption{Normalised execution times for the \spec benchmark suite running under
\mx.}
\label{fig:spec}
\end{figure}

We ran our experiments on a four-core server with 3.50~GHz Intel
Xeon E3-1280 and 16~GB of RAM running 64-bit Linux v3.1.9.

\paragraph{\spec.}
To measure the performance overhead of our prototype, we first used
the standard \spec\footnote{\url{http://www.spec.org/cpu2006/}}
benchmark suite.  Figure~\ref{fig:spec} shows the performance of \mx
running two instances of the same application in parallel, compared to
a native system. The execution time overhead of \mx varies
from \minOverSPEC to \maxOverSPEC compared to executing just a single
version, with the geometric mean across all \numSPECbench benchmarks at
\avgOverSPEC. This result is comparable with previous work using multi-variant
execution that used SPEC CPU to measure performance~\cite{orchestra09} (even
though this work used SPEC~CPU2000 which has already been retired).

%% The overhead varies from \minRedisOver to \maxRedisOver depending
%% on the operation being performed. This is the worst case overhead
%% we have seen among all tested application and comes mainly from the
%% fact that \redis is an in-memory database optimised for maximum
%% bare-hardware performance and is very sensitive to any additional
%% software layers.  Even a state-of-the-art hypervisor can incur an
%% $n$-fold slowdown, so the relatively high measure overhead is
%% therefore unsurprising.

\paragraph{\gnu~\coreutils.} The six \coreutils applications discussed in 
\sref{sec:coreutils} are mostly used in an interactive fashion via the
command-line interface (CLI). For such applications, a high performance
overhead is acceptable as long as it is not perceptible to the user;
prior studies have shown that response times of less than 100ms
typically feel instantaneous~\cite{card:human_proc}. In many common use
cases (\eg creating a directory, or using \cut on a small text file),
the overhead of \mx was imperceptible---\eg creating a directory takes
around \avgMkdirNative natively and \avgMkdirMx with \mx. For the three
utilities that process files, we calculated the maximum file size for
which the response time with \mx stays under the 100ms threshold.  For
\cut, the maximum file size is \cutCutoffSize (with an overhead of
\cutCutoffOver), for \mdsum \mdsumCutoffSize (\mdsumCutoffOver
overhead), and for \shasum \shasumCutoffSize (\shasumCutoffOver
overhead).



\paragraph{\redis and \lighttpd.} To measure the performance overhead for \redis, 
we used
the \redisbenchmark\footnote{\url{http://redis.io/topics/benchmarks}}
utility, which is part of the standard \redis distribution and
simulates \textstt{GET}/\textstt{SET} operations done by $N$ clients
concurrently, with default workload.  For \lighttpd, we used the
\httpload\footnote{\url{http://www.acme.com/software/http_load/}}
multiprocessing test client that is also used by the \lighttpd
developers.  Both of these standard benchmarks measure the end-to-end
time as perceived by users.  As a result, we performed two sets of
experiments: (1) with the client and server located on the same
machine, which represents the worst case performance-wise for \mx; and
(2) with the client and server located on different continents (one in
England and the other in California), which represents the best case.

The overhead for \redis varies, depending on the operation being
performed, from \minRedisRemote to \maxRedisRemote in the remote
scenario, and from \minRedisOver to \maxRedisOver in the local
scenario.  The overhead for \lighttpd varies from \minLighttpdRemote
to \maxLighttpdRemote in the remote scenario, and
from \minLighttpdOver to \maxLighttpdOver in the local scenario.
Despite the relatively large overhead in the local experiments, the
remote overhead is negligible because times are dominated by the
network latency (which in our case is over $150$ms).

As a result, we believe \mx is most suitable for scenarios for which
its execution overhead does not degrade the performance of the
end-to-end task, such as the remote \redis and \lighttpd scenarios
discussed above, or interactive tasks such as those performed using
command-line utilities, where users would not notice the overhead as
long as the response time stays within a certain range.

%% \mx's performance overhead is strongly correlated with the frequency of
%% system calls that have to be intercepted.  Therefore, we could also
%% imagine \mx being automatically turned on and off during execution,
%% depending on the frequency of system calls experienced by the
%% application.

Finally, we would like to emphasise that our current prototype has not
been optimised for performance, and we believe its overhead can still
be significantly reduced.  
%% There are multiple strategies we plan to explore in future
%% work. First,
For example, we could synchronise versions at a coarser granularity,
by using an epoch-based approach~\cite{compl-schedules11}, or we could
improve our checkpointing mechanism by implementing it as a loadable
kernel module that only stores the part of the state needed for
recovery~\cite{flashback}.

%% and only checkpoint at epoch boundaries.  To make this approach viable, we also
%% need to record system calls in each epoch, so that they can be
%% replayed during recovery. Second, 

%% instead of using \textstt{clone}
%% directly, we could implement the checkpointing functionality as a
%% loadable kernel module and only store the part of the state needed for
%% recovery as in~\cite{flashback}. Finally, we could explore the
%% possibility of not intercepting system calls in certain parts of the
%% code that were previously shown to be safe and do not need to be
%% replicated across multiple versions (\eg similarly
%% to~\cite{onlinevalidation}).

%The measured overhead is higher than
%for the SPEC~CPU2006 benchmarks (with a slowdown of up to \maxRedisOver
%for some operations in \redis) and we are currently investigating the
%reasons for this slowdown.

%First, we could to synchronise versions at a coarser
%granularity, by using a window/epoch approach~\cite{compl-schedules11},
%and by performing certain synchronisations at the level of shared
%library calls.  Second, we could explore the possibility of not
%intercepting system calls in certain parts of the code that were
%previously shown to be safe and do not need to be replicated across
%multiple versions.  Finally, we could decrease the checkpointing
%overhead, by performing them at a lower frequency, and record the
%external behaviour since the last checkpoint, so that it can be
%successfully replayed during recovery (\eg as in Rx~\cite{rx}).

\begin{figure}[ht]
\begin{center}
\includegraphics[angle=270,width=\textwidth]{safe-updates/graphs/syscall}
\caption{Number of system calls made on average each second during the
execution of SPEC~CPU2006 benchmark suite, measured using \textstt{strace}
tool.}
\label{fig:syscall}
\end{center}
\end{figure}

We also examined how frequency of system calls affects the performance
overhead of application executed on top \mx. Figure~\ref{fig:syscall} shows
the average number of system calls during the execution of SPEC~CPU2006.
Rather surprising result is the fact that \textsf{452.libquantum}, even though having
the largest run time overhead had the lowest average number of system calls.
On the hand, the performance overhead of \textsf{400.perlbench}, despite having the
highest average number system, was bellow average. \todo{What is the conclusion here?}

% For example, as we discuss in related work, our
% monitor \mxm is very similar to the monitor used by
% Orchestra~\cite{orchestra09}, which by employing various optimisations
% manages to obtain an average overhead of only about 15\% when
% synchronising two program variants at the level of system calls.  In
% terms of checkpointing, the Rx system~\cite{rx} implements a similar
% approach based on the Linux copy-on-write mechanism, and which through
% various optimisations manages to achieve a performance penalty of less
% than 5\% when checkpointing every 200 milliseconds.
